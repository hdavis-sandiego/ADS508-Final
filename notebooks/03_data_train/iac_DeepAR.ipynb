{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4fe966-6e9d-4c11-ab39-0e2efdfbdf1c",
   "metadata": {},
   "source": [
    "# DeepAR Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2746d846-6958-47cc-9cdc-98d53e19e6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a674df7-1f48-4ab9-beba-26ac5a8046ad",
   "metadata": {},
   "source": [
    "## Get train/test dataframes from S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "330cea04-2dc6-4d34-9797-edf913ff67c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aurelia-resort-data/model_train/data_csv/train_nans.csv to data/train_nans.csv\n",
      "download: s3://aurelia-resort-data/model_train/data_csv/train.csv to data/train.csv\n",
      "download: s3://aurelia-resort-data/model_train/data_csv/test_nans.csv to data/test_nans.csv\n",
      "download: s3://aurelia-resort-data/model_train/data_csv/test.csv to data/test.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://aurelia-resort-data/model_train/data_csv ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c00183d-7d97-4e5a-b027-9a2f3bbb3f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_nans.csv\")\n",
    "test = pd.read_csv(\"data/test_nans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18d7e653-a459-4de0-8481-5ffde68c1cff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>num_stays</th>\n",
       "      <th>TotalAirlineTripstoDC</th>\n",
       "      <th>TotalAirlinePassengerstoDC</th>\n",
       "      <th>TotalAmericanTravelers</th>\n",
       "      <th>PercentofAmericanswhoTraveled</th>\n",
       "      <th>TotalTripsbyAmericans</th>\n",
       "      <th>income_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>4413</td>\n",
       "      <td>830.0</td>\n",
       "      <td>2139952.0</td>\n",
       "      <td>2.601098e+08</td>\n",
       "      <td>82.156684</td>\n",
       "      <td>1.210049e+09</td>\n",
       "      <td>15125.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>7754</td>\n",
       "      <td>896.0</td>\n",
       "      <td>3059442.0</td>\n",
       "      <td>2.621167e+08</td>\n",
       "      <td>81.897555</td>\n",
       "      <td>1.218480e+09</td>\n",
       "      <td>15064.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>9208</td>\n",
       "      <td>858.0</td>\n",
       "      <td>3272280.0</td>\n",
       "      <td>2.614628e+08</td>\n",
       "      <td>81.906942</td>\n",
       "      <td>1.279973e+09</td>\n",
       "      <td>15055.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>10146</td>\n",
       "      <td>902.0</td>\n",
       "      <td>3415990.0</td>\n",
       "      <td>2.610540e+08</td>\n",
       "      <td>82.059921</td>\n",
       "      <td>1.305916e+09</td>\n",
       "      <td>15036.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>8271</td>\n",
       "      <td>938.0</td>\n",
       "      <td>3298350.0</td>\n",
       "      <td>2.601869e+08</td>\n",
       "      <td>81.819188</td>\n",
       "      <td>1.236950e+09</td>\n",
       "      <td>14973.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  num_stays  TotalAirlineTripstoDC  TotalAirlinePassengerstoDC  \\\n",
       "152 2022-02-01       4413                  830.0                   2139952.0   \n",
       "153 2022-03-01       7754                  896.0                   3059442.0   \n",
       "154 2022-04-01       9208                  858.0                   3272280.0   \n",
       "155 2022-05-01      10146                  902.0                   3415990.0   \n",
       "156 2022-06-01       8271                  938.0                   3298350.0   \n",
       "\n",
       "     TotalAmericanTravelers  PercentofAmericanswhoTraveled  \\\n",
       "152            2.601098e+08                      82.156684   \n",
       "153            2.621167e+08                      81.897555   \n",
       "154            2.614628e+08                      81.906942   \n",
       "155            2.610540e+08                      82.059921   \n",
       "156            2.601869e+08                      81.819188   \n",
       "\n",
       "     TotalTripsbyAmericans  income_total  \n",
       "152           1.210049e+09       15125.6  \n",
       "153           1.218480e+09       15064.1  \n",
       "154           1.279973e+09       15055.2  \n",
       "155           1.305916e+09       15036.4  \n",
       "156           1.236950e+09       14973.1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09a7eabc-5e17-4cb5-908e-76d8f86d3472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>num_stays</th>\n",
       "      <th>TotalAirlineTripstoDC</th>\n",
       "      <th>TotalAirlinePassengerstoDC</th>\n",
       "      <th>TotalAmericanTravelers</th>\n",
       "      <th>PercentofAmericanswhoTraveled</th>\n",
       "      <th>TotalTripsbyAmericans</th>\n",
       "      <th>income_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>8002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15100.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>7866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15149.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>8091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15172.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>9588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15274.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>6964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15332.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  num_stays  TotalAirlineTripstoDC  TotalAirlinePassengerstoDC  \\\n",
       "0 2022-07-01       8002                    NaN                         NaN   \n",
       "1 2022-08-01       7866                    NaN                         NaN   \n",
       "2 2022-09-01       8091                    NaN                         NaN   \n",
       "3 2022-10-01       9588                    NaN                         NaN   \n",
       "4 2022-11-01       6964                    NaN                         NaN   \n",
       "\n",
       "   TotalAmericanTravelers  PercentofAmericanswhoTraveled  \\\n",
       "0                     NaN                            NaN   \n",
       "1                     NaN                            NaN   \n",
       "2                     NaN                            NaN   \n",
       "3                     NaN                            NaN   \n",
       "4                     NaN                            NaN   \n",
       "\n",
       "   TotalTripsbyAmericans  income_total  \n",
       "0                    NaN       15100.2  \n",
       "1                    NaN       15149.6  \n",
       "2                    NaN       15172.2  \n",
       "3                    NaN       15274.2  \n",
       "4                    NaN       15332.9  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Date'] = pd.to_datetime(test['Date'])\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0878249-b122-4ec5-a0b2-a5a62dc46278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(100)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638edf21-c04c-475e-baae-bc07529ced22",
   "metadata": {},
   "source": [
    "## Create JSON Object for DeepAR Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ca236fd-ef30-4b1e-aea7-9ebc149a0b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Create JSON object by grouping train set by Date\n",
    "train_json = []\n",
    "for Date, group in train.groupby('Date'):\n",
    "    item = {\n",
    "        'start': Date.isoformat(),\n",
    "        'target': group[['TotalAirlineTripstoDC', 'TotalAirlinePassengerstoDC', 'TotalAmericanTravelers', 'PercentofAmericanswhoTraveled', 'TotalTripsbyAmericans', 'num_stays', 'income_total']].values.tolist()\n",
    "        #'dynamic_feat': group.drop(['Date', 'TotalAirlineTripstoDC', 'TotalAirlinePassengerstoDC', 'TotalAmericanTravelers', 'PercentofAmericanswhoTraveled', 'TotalTripsbyAmericans', 'num_stays', 'income_total'], axis=1).values.tolist()\n",
    "    }\n",
    "    train_json.append(item)\n",
    "    \n",
    "    \n",
    "# Create JSON object by grouping test set by Date\n",
    "test_json = []\n",
    "for Date, group in test.groupby('Date'):\n",
    "    item = {\n",
    "        'start': Date.isoformat(),\n",
    "        'target': group[['TotalAirlineTripstoDC', 'TotalAirlinePassengerstoDC', 'TotalAmericanTravelers', 'PercentofAmericanswhoTraveled', 'TotalTripsbyAmericans', 'num_stays', 'income_total']].values.tolist()\n",
    "        #'dynamic_feat': group.drop(['Date', 'TotalAirlineTripstoDC', 'TotalAirlinePassengerstoDC', 'TotalAmericanTravelers', 'PercentofAmericanswhoTraveled', 'TotalTripsbyAmericans', 'num_stays', 'income_total'], axis=1).values.tolist()\n",
    "    }\n",
    "    test_json.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92ae764a-2f73-43ab-ac7c-7b2c42fd940d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(train_json))\n",
    "print(len(test_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82079fce-7ca2-46d9-934c-65dbaf4de574",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': '2009-06-01T00:00:00',\n",
       " 'target': [[nan, nan, nan, nan, nan, 2.0, nan]]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_json[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d837211-3138-4320-837b-7c0531debb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ed74802-3ee6-4401-acef-f281ae892b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 ms, sys: 206 µs, total: 3.61 ms\n",
      "Wall time: 43.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", train_json)\n",
    "write_dicts_to_file(\"test.json\", test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dea786-11a1-47b4-afa8-4b57e485bf93",
   "metadata": {},
   "source": [
    "## Train Model with DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04d07671-c117-4aa9-8338-0ac11db3b23c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_data_path = \"s3://aurelia-resort-data/model_train/deepAR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7dcbbb07-07f3-429f-85c5-4cd989dbeb31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    base_job_name=\"deepar-hotel\",\n",
    "    output_path=\"s3://aurelia-resort-data/model_train/deepAR/output/\",\n",
    "    hyperparameters={\n",
    "        \"time_freq\": \"M\",\n",
    "        \"prediction_length\": \"2\",\n",
    "        \"context_length\": \"3\",\n",
    "        \"num_cells\": \"40\",\n",
    "        \"num_layers\": \"2\",\n",
    "        \"likelihood\": \"student-t\",\n",
    "        \"epochs\": \"200\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d412843-49ea-4c1a-99c6-cc95be15731f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: deepar-hotel-2023-04-03-23-32-55-031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 23:33:00 Starting - Starting the training job...\n",
      "2023-04-03 23:33:26 Starting - Preparing the instances for training......\n",
      "2023-04-03 23:34:18 Downloading - Downloading input data...\n",
      "2023-04-03 23:34:44 Training - Downloading the training image...\n",
      "2023-04-03 23:35:34 Training - Training image download completed. Training in progress...\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:78: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '3', 'epochs': '200', 'likelihood': 'student-t', 'num_cells': '40', 'num_layers': '2', 'prediction_length': '2', 'time_freq': 'M'}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '3', 'epochs': '200', 'prediction_length': '2', 'time_freq': 'M'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] random_seed is None\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Real time series\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] number of time series: 157\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] number of observations: 1099\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] mean target length: 7.0\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] min/mean/max target: 0.0/41779158.800997175/1476232320.0\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] mean abs(target): 41779158.800997175\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] contains missing values: yes (61.8%)\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Small number of time series. Doing 9 passes over dataset with prob 0.9058740268931351 per epoch.\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Real time series\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] number of time series: 6\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] number of observations: 42\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] mean target length: 7.0\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] min/mean/max target: 0.0/3193.2952008928573/15367.2998046875\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] mean abs(target): 3193.2952008928573\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] contains missing values: yes (71.4%)\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] #memory_usage::<batchbuffer> = 0.2294921875 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] nvidia-smi: took 0.029 seconds to run.\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564943.7307603, \"EndTime\": 1680564943.7496126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 17.528295516967773, \"count\": 1, \"min\": 17.528295516967773, \"max\": 17.528295516967773}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:43 INFO 140244386916160] #memory_usage::<model> = 4 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564943.7496862, \"EndTime\": 1680564943.7754962, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 44.62718963623047, \"count\": 1, \"min\": 44.62718963623047, \"max\": 44.62718963623047}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[0] Batch[0] avg_epoch_loss=4.163423\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.163422584533691\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[0] Batch[5] avg_epoch_loss=4.674401\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=4.674400965372722\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[0] Batch [5]#011Speed: 9696.38 samples/sec#011loss=4.674401\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[0] Batch[10] avg_epoch_loss=4.592759\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=4.494788074493409\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[0] Batch [10]#011Speed: 5026.34 samples/sec#011loss=4.494788\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] processed a total of 1502 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564943.7756999, \"EndTime\": 1680564944.2404423, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 464.58888053894043, \"count\": 1, \"min\": 464.58888053894043, \"max\": 464.58888053894043}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3231.9541482128648 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=0, train loss <loss>=5.046265244483948\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[1] Batch[0] avg_epoch_loss=4.696948\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=4.6969475746154785\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[1] Batch[5] avg_epoch_loss=4.565127\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=4.565126856168111\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[1] Batch [5]#011Speed: 9658.52 samples/sec#011loss=4.565127\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[1] Batch[10] avg_epoch_loss=4.441208\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=4.292504596710205\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[1] Batch [10]#011Speed: 5926.08 samples/sec#011loss=4.292505\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] processed a total of 1538 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564944.2405457, \"EndTime\": 1680564944.671412, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 430.3708076477051, \"count\": 1, \"min\": 430.3708076477051, \"max\": 430.3708076477051}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3572.4314802672384 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=1, train loss <loss>=4.201402095647959\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[2] Batch[0] avg_epoch_loss=3.268163\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.2681632041931152\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[2] Batch[5] avg_epoch_loss=4.193012\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=4.19301164150238\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:44 INFO 140244386916160] Epoch[2] Batch [5]#011Speed: 7670.81 samples/sec#011loss=4.193012\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[2] Batch[10] avg_epoch_loss=4.300104\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=4.428614521026612\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[2] Batch [10]#011Speed: 4696.80 samples/sec#011loss=4.428615\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] processed a total of 1579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564944.671512, \"EndTime\": 1680564945.1466246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 474.6119976043701, \"count\": 1, \"min\": 474.6119976043701, \"max\": 474.6119976043701}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3325.882236315801 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=2, train loss <loss>=4.155539173346299\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[3] Batch[0] avg_epoch_loss=5.172254\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=5.172253608703613\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[3] Batch[5] avg_epoch_loss=4.384604\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=4.38460377852122\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[3] Batch [5]#011Speed: 9514.26 samples/sec#011loss=4.384604\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[3] Batch[10] avg_epoch_loss=4.493007\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=4.623091030120849\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[3] Batch [10]#011Speed: 6058.41 samples/sec#011loss=4.623091\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] processed a total of 1529 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564945.1467187, \"EndTime\": 1680564945.575829, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 428.67398262023926, \"count\": 1, \"min\": 428.67398262023926, \"max\": 428.67398262023926}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3565.7957655871387 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=3, train loss <loss>=4.7794626752535505\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[4] Batch[0] avg_epoch_loss=5.829015\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=5.829015254974365\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[4] Batch[5] avg_epoch_loss=4.515656\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=4.515655795733134\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[4] Batch [5]#011Speed: 9596.23 samples/sec#011loss=4.515656\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[4] Batch[10] avg_epoch_loss=4.555674\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=4.603695583343506\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:45 INFO 140244386916160] Epoch[4] Batch [10]#011Speed: 5332.90 samples/sec#011loss=4.603696\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] processed a total of 1573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564945.5759006, \"EndTime\": 1680564946.0283623, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 451.94506645202637, \"count\": 1, \"min\": 451.94506645202637, \"max\": 451.94506645202637}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3479.3147685929102 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=4, train loss <loss>=4.347053949649517\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[5] Batch[0] avg_epoch_loss=4.434493\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=4.434493064880371\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[5] Batch[5] avg_epoch_loss=4.536685\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=4.53668475151062\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[5] Batch [5]#011Speed: 8981.56 samples/sec#011loss=4.536685\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[5] Batch[10] avg_epoch_loss=4.500510\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=4.4571013927459715\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[5] Batch [10]#011Speed: 5158.47 samples/sec#011loss=4.457101\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] processed a total of 1576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564946.0284646, \"EndTime\": 1680564946.4818978, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.90350914001465, \"count\": 1, \"min\": 452.90350914001465, \"max\": 452.90350914001465}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3478.6052495170375 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=5, train loss <loss>=4.223552181170537\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[6] Batch[0] avg_epoch_loss=4.366284\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=4.366284370422363\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[6] Batch[5] avg_epoch_loss=4.356759\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=4.356759190559387\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[6] Batch [5]#011Speed: 7106.94 samples/sec#011loss=4.356759\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[6] Batch[10] avg_epoch_loss=4.595470\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=4.881922912597656\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] Epoch[6] Batch [10]#011Speed: 4493.49 samples/sec#011loss=4.881923\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] processed a total of 1473 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564946.4819977, \"EndTime\": 1680564946.994472, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 511.9452476501465, \"count\": 1, \"min\": 511.9452476501465, \"max\": 511.9452476501465}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2876.2401173920493 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=6, train loss <loss>=5.120041271050771\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] Epoch[7] Batch[0] avg_epoch_loss=3.499750\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.4997503757476807\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] Epoch[7] Batch[5] avg_epoch_loss=4.084109\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.0841091076533\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] Epoch[7] Batch [5]#011Speed: 8285.84 samples/sec#011loss=4.084109\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] Epoch[7] Batch[10] avg_epoch_loss=4.069682\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=4.052369785308838\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] Epoch[7] Batch [10]#011Speed: 4582.43 samples/sec#011loss=4.052370\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] processed a total of 1543 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564946.9946036, \"EndTime\": 1680564947.5648887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.6208477020264, \"count\": 1, \"min\": 569.6208477020264, \"max\": 569.6208477020264}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2708.1551543697688 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.7701660360281286\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] Epoch[8] Batch[0] avg_epoch_loss=4.361392\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=4.361392021179199\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] Epoch[8] Batch[5] avg_epoch_loss=3.927531\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=3.9275309642155967\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:47 INFO 140244386916160] Epoch[8] Batch [5]#011Speed: 8282.16 samples/sec#011loss=3.927531\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] Epoch[8] Batch[10] avg_epoch_loss=4.186540\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=4.497349834442138\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] Epoch[8] Batch [10]#011Speed: 3973.99 samples/sec#011loss=4.497350\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] processed a total of 1590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564947.5649796, \"EndTime\": 1680564948.1431286, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.6891708374023, \"count\": 1, \"min\": 577.6891708374023, \"max\": 577.6891708374023}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2751.676275678301 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=8, train loss <loss>=4.138870991193331\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] Epoch[9] Batch[0] avg_epoch_loss=4.145160\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=4.145159721374512\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] Epoch[9] Batch[5] avg_epoch_loss=3.953814\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.9538137912750244\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] Epoch[9] Batch [5]#011Speed: 6709.31 samples/sec#011loss=3.953814\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] Epoch[9] Batch[10] avg_epoch_loss=4.329533\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=4.780397129058838\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] Epoch[9] Batch [10]#011Speed: 4104.96 samples/sec#011loss=4.780397\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] processed a total of 1525 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564948.1432204, \"EndTime\": 1680564948.7224836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 578.8190364837646, \"count\": 1, \"min\": 578.8190364837646, \"max\": 578.8190364837646}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2633.6909805859223 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=9, train loss <loss>=4.386096318562825\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] Epoch[10] Batch[0] avg_epoch_loss=2.955300\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=2.9552996158599854\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[10] Batch[5] avg_epoch_loss=3.935120\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=3.935120145479838\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[10] Batch [5]#011Speed: 8759.58 samples/sec#011loss=3.935120\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[10] Batch[10] avg_epoch_loss=3.944154\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=3.954995536804199\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[10] Batch [10]#011Speed: 5838.23 samples/sec#011loss=3.954996\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] processed a total of 1548 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564948.7226503, \"EndTime\": 1680564949.1678317, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 444.48304176330566, \"count\": 1, \"min\": 444.48304176330566, \"max\": 444.48304176330566}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3481.6461246420645 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.778661792094891\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[11] Batch[0] avg_epoch_loss=4.205031\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=4.205031394958496\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[11] Batch[5] avg_epoch_loss=4.001133\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=4.001132885615031\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[11] Batch [5]#011Speed: 9381.59 samples/sec#011loss=4.001133\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[11] Batch[10] avg_epoch_loss=4.262170\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=4.575413942337036\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[11] Batch [10]#011Speed: 5253.86 samples/sec#011loss=4.575414\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] processed a total of 1547 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564949.1679142, \"EndTime\": 1680564949.613771, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 445.33514976501465, \"count\": 1, \"min\": 445.33514976501465, \"max\": 445.33514976501465}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3472.598388770524 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=11, train loss <loss>=4.003461457215822\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[12] Batch[0] avg_epoch_loss=4.672822\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=4.672821998596191\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[12] Batch[5] avg_epoch_loss=4.126452\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=4.1264519691467285\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:49 INFO 140244386916160] Epoch[12] Batch [5]#011Speed: 7964.71 samples/sec#011loss=4.126452\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[12] Batch[10] avg_epoch_loss=4.084819\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=4.03485860824585\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[12] Batch [10]#011Speed: 5360.48 samples/sec#011loss=4.034859\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] processed a total of 1582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564949.613874, \"EndTime\": 1680564950.0774958, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 463.0565643310547, \"count\": 1, \"min\": 463.0565643310547, \"max\": 463.0565643310547}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3415.3196690808413 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=12, train loss <loss>=4.856726426344651\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[13] Batch[0] avg_epoch_loss=3.950321\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.9503209590911865\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[13] Batch[5] avg_epoch_loss=4.639170\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=4.6391704479853315\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[13] Batch [5]#011Speed: 9546.24 samples/sec#011loss=4.639170\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[13] Batch[10] avg_epoch_loss=4.711469\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=4.798226642608642\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[13] Batch [10]#011Speed: 4554.72 samples/sec#011loss=4.798227\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] processed a total of 1609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564950.0775883, \"EndTime\": 1680564950.5425768, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.585542678833, \"count\": 1, \"min\": 464.585542678833, \"max\": 464.585542678833}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3462.3601941981683 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=13, train loss <loss>=5.054263720145593\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[14] Batch[0] avg_epoch_loss=3.117178\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.117177724838257\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[14] Batch[5] avg_epoch_loss=3.770004\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=3.7700035572052\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[14] Batch [5]#011Speed: 8377.56 samples/sec#011loss=3.770004\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[14] Batch[10] avg_epoch_loss=3.948241\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=4.162126541137695\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:50 INFO 140244386916160] Epoch[14] Batch [10]#011Speed: 5509.56 samples/sec#011loss=4.162127\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] processed a total of 1503 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564950.54266, \"EndTime\": 1680564951.007331, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.0796184539795, \"count\": 1, \"min\": 464.0796184539795, \"max\": 464.0796184539795}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3237.5157725207273 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.9421812494595847\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[15] Batch[0] avg_epoch_loss=3.755219\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.755218505859375\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[15] Batch[5] avg_epoch_loss=4.105313\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=4.105312983194987\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[15] Batch [5]#011Speed: 9421.96 samples/sec#011loss=4.105313\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[15] Batch[10] avg_epoch_loss=4.269514\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=4.466555261611939\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[15] Batch [10]#011Speed: 5274.61 samples/sec#011loss=4.466555\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] processed a total of 1504 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564951.0074465, \"EndTime\": 1680564951.4429827, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 434.9031448364258, \"count\": 1, \"min\": 434.9031448364258, \"max\": 434.9031448364258}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3457.020767662597 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=15, train loss <loss>=4.3979329864184065\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[16] Batch[0] avg_epoch_loss=4.711110\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=4.711109638214111\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[16] Batch[5] avg_epoch_loss=3.804246\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.8042461474736533\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[16] Batch [5]#011Speed: 9169.85 samples/sec#011loss=3.804246\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[16] Batch[10] avg_epoch_loss=4.076676\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=4.403592300415039\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] Epoch[16] Batch [10]#011Speed: 5203.93 samples/sec#011loss=4.403592\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] processed a total of 1487 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564951.4430766, \"EndTime\": 1680564951.8863292, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 442.74401664733887, \"count\": 1, \"min\": 442.74401664733887, \"max\": 442.74401664733887}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3357.4536079976874 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=16, train loss <loss>=4.014496187369029\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[17] Batch[0] avg_epoch_loss=3.852066\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=3.8520660400390625\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[17] Batch[5] avg_epoch_loss=4.257784\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=4.257783770561218\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[17] Batch [5]#011Speed: 9423.45 samples/sec#011loss=4.257784\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[17] Batch[10] avg_epoch_loss=4.097522\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=3.9052069187164307\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[17] Batch [10]#011Speed: 5916.05 samples/sec#011loss=3.905207\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] processed a total of 1523 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564951.8864253, \"EndTime\": 1680564952.3056498, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 418.6842441558838, \"count\": 1, \"min\": 418.6842441558838, \"max\": 418.6842441558838}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3636.312658250906 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=17, train loss <loss>=4.2971014976501465\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[18] Batch[0] avg_epoch_loss=3.589288\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.589287519454956\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[18] Batch[5] avg_epoch_loss=3.955169\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.9551687240600586\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[18] Batch [5]#011Speed: 8346.02 samples/sec#011loss=3.955169\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[18] Batch[10] avg_epoch_loss=4.343443\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=4.809372806549073\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[18] Batch [10]#011Speed: 5508.44 samples/sec#011loss=4.809373\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] processed a total of 1530 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564952.3057444, \"EndTime\": 1680564952.7452726, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 439.0413761138916, \"count\": 1, \"min\": 439.0413761138916, \"max\": 439.0413761138916}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3483.781143359069 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=18, train loss <loss>=4.426272948582967\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] Epoch[19] Batch[0] avg_epoch_loss=3.380345\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.380345344543457\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[19] Batch[5] avg_epoch_loss=3.979769\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=3.9797693888346353\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[19] Batch [5]#011Speed: 9113.13 samples/sec#011loss=3.979769\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[19] Batch[10] avg_epoch_loss=4.099414\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=4.242987966537475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[19] Batch [10]#011Speed: 5079.81 samples/sec#011loss=4.242988\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] processed a total of 1574 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564952.7453618, \"EndTime\": 1680564953.2096508, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 463.87600898742676, \"count\": 1, \"min\": 463.87600898742676, \"max\": 463.87600898742676}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3391.617581164286 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=19, train loss <loss>=4.0337145787018995\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[20] Batch[0] avg_epoch_loss=4.963703\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=4.96370267868042\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[20] Batch[5] avg_epoch_loss=4.139446\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=4.139445900917053\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[20] Batch [5]#011Speed: 9680.05 samples/sec#011loss=4.139446\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[20] Batch[10] avg_epoch_loss=4.431543\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=4.782059383392334\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[20] Batch [10]#011Speed: 4764.27 samples/sec#011loss=4.782059\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] processed a total of 1556 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564953.2098086, \"EndTime\": 1680564953.6687112, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 458.3733081817627, \"count\": 1, \"min\": 458.3733081817627, \"max\": 458.3733081817627}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3393.6104122196944 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=20, train loss <loss>=4.4793946743011475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] Epoch[21] Batch[0] avg_epoch_loss=5.003448\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=5.003448486328125\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[21] Batch[5] avg_epoch_loss=4.441499\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=4.4414987961451216\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[21] Batch [5]#011Speed: 7800.57 samples/sec#011loss=4.441499\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[21] Batch[10] avg_epoch_loss=4.126181\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=3.7478000640869142\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[21] Batch [10]#011Speed: 5044.72 samples/sec#011loss=3.747800\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] processed a total of 1570 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564953.6687987, \"EndTime\": 1680564954.1749754, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 505.7251453399658, \"count\": 1, \"min\": 505.7251453399658, \"max\": 505.7251453399658}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3103.089670264837 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=21, train loss <loss>=4.439708177859966\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[22] Batch[0] avg_epoch_loss=5.449825\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=5.449825286865234\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[22] Batch[5] avg_epoch_loss=4.470625\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=4.470625241597493\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[22] Batch [5]#011Speed: 7786.38 samples/sec#011loss=4.470625\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[22] Batch[10] avg_epoch_loss=4.401429\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=4.318392515182495\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[22] Batch [10]#011Speed: 4968.50 samples/sec#011loss=4.318393\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] processed a total of 1572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564954.1751463, \"EndTime\": 1680564954.6467826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 471.07458114624023, \"count\": 1, \"min\": 471.07458114624023, \"max\": 471.07458114624023}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3336.0381536406553 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=22, train loss <loss>=4.297045074976408\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[23] Batch[0] avg_epoch_loss=3.336723\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.3367228507995605\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[23] Batch[5] avg_epoch_loss=4.244788\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=4.244787573814392\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:54 INFO 140244386916160] Epoch[23] Batch [5]#011Speed: 9546.65 samples/sec#011loss=4.244788\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[23] Batch[10] avg_epoch_loss=4.157550\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=4.052865505218506\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[23] Batch [10]#011Speed: 5568.90 samples/sec#011loss=4.052866\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] processed a total of 1487 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564954.6468687, \"EndTime\": 1680564955.0969708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.6595859527588, \"count\": 1, \"min\": 449.6595859527588, \"max\": 449.6595859527588}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3305.833028382673 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=23, train loss <loss>=4.109959046045939\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[24] Batch[0] avg_epoch_loss=4.119627\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=4.119627475738525\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[24] Batch[5] avg_epoch_loss=4.159347\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=4.1593473354975385\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[24] Batch [5]#011Speed: 9621.17 samples/sec#011loss=4.159347\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[24] Batch[10] avg_epoch_loss=4.215734\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=4.283398962020874\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[24] Batch [10]#011Speed: 4888.31 samples/sec#011loss=4.283399\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] processed a total of 1554 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564955.0970612, \"EndTime\": 1680564955.5615418, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.02478218078613, \"count\": 1, \"min\": 464.02478218078613, \"max\": 464.02478218078613}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3347.8702055684594 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=24, train loss <loss>=4.003605851760278\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[25] Batch[0] avg_epoch_loss=4.295694\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=4.295694351196289\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[25] Batch[5] avg_epoch_loss=4.476242\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=4.476241747538249\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:55 INFO 140244386916160] Epoch[25] Batch [5]#011Speed: 7779.07 samples/sec#011loss=4.476242\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[25] Batch[10] avg_epoch_loss=4.321606\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=4.136043214797974\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[25] Batch [10]#011Speed: 4394.33 samples/sec#011loss=4.136043\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] processed a total of 1617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564955.5616446, \"EndTime\": 1680564956.05652, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 494.4436550140381, \"count\": 1, \"min\": 494.4436550140381, \"max\": 494.4436550140381}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3269.478362606929 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=25, train loss <loss>=4.34211410008944\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[26] Batch[0] avg_epoch_loss=4.064168\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=4.0641679763793945\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[26] Batch[5] avg_epoch_loss=4.018303\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=4.018302798271179\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[26] Batch [5]#011Speed: 8062.50 samples/sec#011loss=4.018303\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[26] Batch[10] avg_epoch_loss=4.290268\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=4.616626596450805\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[26] Batch [10]#011Speed: 6048.91 samples/sec#011loss=4.616627\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] processed a total of 1483 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564956.056602, \"EndTime\": 1680564956.4956334, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 438.5497570037842, \"count\": 1, \"min\": 438.5497570037842, \"max\": 438.5497570037842}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3380.5601004795167 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=26, train loss <loss>=4.121306995550792\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[27] Batch[0] avg_epoch_loss=3.857479\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.8574793338775635\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[27] Batch[5] avg_epoch_loss=4.224316\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=4.224315961201985\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[27] Batch [5]#011Speed: 9634.81 samples/sec#011loss=4.224316\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[27] Batch[10] avg_epoch_loss=3.973708\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=3.6729781150817873\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] Epoch[27] Batch [10]#011Speed: 5143.94 samples/sec#011loss=3.672978\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] processed a total of 1566 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564956.4957209, \"EndTime\": 1680564956.9652638, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 469.1133499145508, \"count\": 1, \"min\": 469.1133499145508, \"max\": 469.1133499145508}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3337.0709834032505 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=27, train loss <loss>=3.9093359250288744\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[28] Batch[0] avg_epoch_loss=4.581285\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=4.58128547668457\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[28] Batch[5] avg_epoch_loss=3.962342\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=3.9623419046401978\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[28] Batch [5]#011Speed: 7847.84 samples/sec#011loss=3.962342\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[28] Batch[10] avg_epoch_loss=4.057018\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=4.170629978179932\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[28] Batch [10]#011Speed: 5565.96 samples/sec#011loss=4.170630\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] processed a total of 1508 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564956.9653656, \"EndTime\": 1680564957.4193494, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 453.4647464752197, \"count\": 1, \"min\": 453.4647464752197, \"max\": 453.4647464752197}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3324.468326147413 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=28, train loss <loss>=4.457121868928273\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[29] Batch[0] avg_epoch_loss=3.954124\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.9541242122650146\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[29] Batch[5] avg_epoch_loss=3.957791\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.957790970802307\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[29] Batch [5]#011Speed: 9415.88 samples/sec#011loss=3.957791\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[29] Batch[10] avg_epoch_loss=4.163831\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=4.411078357696534\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] Epoch[29] Batch [10]#011Speed: 5084.32 samples/sec#011loss=4.411078\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] processed a total of 1547 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564957.4194443, \"EndTime\": 1680564957.8727632, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.8310298919678, \"count\": 1, \"min\": 452.8310298919678, \"max\": 452.8310298919678}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3415.4077815600494 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.9645989445539622\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[30] Batch[0] avg_epoch_loss=3.632527\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.6325268745422363\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[30] Batch[5] avg_epoch_loss=3.557549\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=3.557549317677816\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[30] Batch [5]#011Speed: 8056.70 samples/sec#011loss=3.557549\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[30] Batch[10] avg_epoch_loss=3.961100\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=4.445360326766968\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[30] Batch [10]#011Speed: 5430.15 samples/sec#011loss=4.445360\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] processed a total of 1538 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564957.8728392, \"EndTime\": 1680564958.331625, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 458.2555294036865, \"count\": 1, \"min\": 458.2555294036865, \"max\": 458.2555294036865}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3355.3437177675355 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=30, train loss <loss>=4.234738551653349\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[31] Batch[0] avg_epoch_loss=3.643534\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=3.6435344219207764\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[31] Batch[5] avg_epoch_loss=3.760486\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=3.760485808054606\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[31] Batch [5]#011Speed: 9200.21 samples/sec#011loss=3.760486\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[31] Batch[10] avg_epoch_loss=3.970336\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=4.222157287597656\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] Epoch[31] Batch [10]#011Speed: 5099.76 samples/sec#011loss=4.222157\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] processed a total of 1555 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564958.3316944, \"EndTime\": 1680564958.7938938, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 461.74073219299316, \"count\": 1, \"min\": 461.74073219299316, \"max\": 461.74073219299316}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3366.3471339190505 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=31, train loss <loss>=3.8043708434471717\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[32] Batch[0] avg_epoch_loss=3.976801\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=3.9768011569976807\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[32] Batch[5] avg_epoch_loss=4.141304\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=4.141303539276123\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[32] Batch [5]#011Speed: 8414.85 samples/sec#011loss=4.141304\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[32] Batch[10] avg_epoch_loss=4.312061\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=4.516969203948975\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[32] Batch [10]#011Speed: 6303.30 samples/sec#011loss=4.516969\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] processed a total of 1465 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564958.7940302, \"EndTime\": 1680564959.2436001, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.0697383880615, \"count\": 1, \"min\": 449.0697383880615, \"max\": 449.0697383880615}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3261.153350737741 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=32, train loss <loss>=4.118415216604869\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[33] Batch[0] avg_epoch_loss=3.435959\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=3.4359588623046875\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[33] Batch[5] avg_epoch_loss=3.621937\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.6219367583592734\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[33] Batch [5]#011Speed: 9351.07 samples/sec#011loss=3.621937\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[33] Batch[10] avg_epoch_loss=3.877007\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=4.183090448379517\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[33] Batch [10]#011Speed: 4950.96 samples/sec#011loss=4.183090\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] processed a total of 1536 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564959.2436976, \"EndTime\": 1680564959.6929243, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 448.70924949645996, \"count\": 1, \"min\": 448.70924949645996, \"max\": 448.70924949645996}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3422.1065485037166 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=33, train loss <loss>=4.001751025517781\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] Epoch[34] Batch[0] avg_epoch_loss=2.449584\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:35:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.4495842456817627\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[34] Batch[5] avg_epoch_loss=3.289838\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=3.289837598800659\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[34] Batch [5]#011Speed: 8527.01 samples/sec#011loss=3.289838\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[34] Batch[10] avg_epoch_loss=3.840300\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=4.500853824615478\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[34] Batch [10]#011Speed: 5235.60 samples/sec#011loss=4.500854\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] processed a total of 1484 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564959.6930144, \"EndTime\": 1680564960.172041, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 478.55520248413086, \"count\": 1, \"min\": 478.55520248413086, \"max\": 478.55520248413086}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3100.1511817173005 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.8044638435045877\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[35] Batch[0] avg_epoch_loss=4.323123\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=4.323122501373291\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[35] Batch[5] avg_epoch_loss=4.315671\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=4.3156706889470415\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[35] Batch [5]#011Speed: 9358.53 samples/sec#011loss=4.315671\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[35] Batch[10] avg_epoch_loss=4.399786\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=4.500724220275879\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[35] Batch [10]#011Speed: 5463.32 samples/sec#011loss=4.500724\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] processed a total of 1535 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564960.172127, \"EndTime\": 1680564960.6625297, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 489.9415969848633, \"count\": 1, \"min\": 489.9415969848633, \"max\": 489.9415969848633}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3132.0403714516024 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=35, train loss <loss>=4.553812325000763\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[36] Batch[0] avg_epoch_loss=4.033994\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=4.033993721008301\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[36] Batch[5] avg_epoch_loss=4.213055\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=4.213054696718852\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:00 INFO 140244386916160] Epoch[36] Batch [5]#011Speed: 8822.13 samples/sec#011loss=4.213055\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] Epoch[36] Batch[10] avg_epoch_loss=4.231703\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=4.254081392288208\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] Epoch[36] Batch [10]#011Speed: 4877.07 samples/sec#011loss=4.254081\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] processed a total of 1461 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564960.6626348, \"EndTime\": 1680564961.1334891, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 470.34263610839844, \"count\": 1, \"min\": 470.34263610839844, \"max\": 470.34263610839844}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3105.2167178301156 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=36, train loss <loss>=4.062595109144847\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] Epoch[37] Batch[0] avg_epoch_loss=3.934739\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=3.9347386360168457\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] Epoch[37] Batch[5] avg_epoch_loss=3.694982\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.694981892903646\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] Epoch[37] Batch [5]#011Speed: 8479.12 samples/sec#011loss=3.694982\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] Epoch[37] Batch[10] avg_epoch_loss=3.793306\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=3.911295223236084\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] Epoch[37] Batch [10]#011Speed: 4467.53 samples/sec#011loss=3.911295\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] processed a total of 1555 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564961.133591, \"EndTime\": 1680564961.6797955, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 545.6726551055908, \"count\": 1, \"min\": 545.6726551055908, \"max\": 545.6726551055908}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2848.9706102045257 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=37, train loss <loss>=3.6108402151327867\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] Epoch[38] Batch[0] avg_epoch_loss=4.146186\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=4.146186351776123\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[38] Batch[5] avg_epoch_loss=4.002272\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=4.002272288004558\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[38] Batch [5]#011Speed: 8275.08 samples/sec#011loss=4.002272\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[38] Batch[10] avg_epoch_loss=4.209555\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=4.458293628692627\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[38] Batch [10]#011Speed: 4733.64 samples/sec#011loss=4.458294\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] processed a total of 1547 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564961.6798859, \"EndTime\": 1680564962.219341, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.9959812164307, \"count\": 1, \"min\": 538.9959812164307, \"max\": 538.9959812164307}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2869.218433499245 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=38, train loss <loss>=4.492553197420561\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[39] Batch[0] avg_epoch_loss=3.730991\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=3.7309911251068115\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[39] Batch[5] avg_epoch_loss=3.993554\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=3.99355411529541\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[39] Batch [5]#011Speed: 7349.50 samples/sec#011loss=3.993554\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[39] Batch[10] avg_epoch_loss=4.314566\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=4.6997802734375\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[39] Batch [10]#011Speed: 6253.80 samples/sec#011loss=4.699780\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] processed a total of 1478 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564962.2194533, \"EndTime\": 1680564962.6698682, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.026273727417, \"count\": 1, \"min\": 450.026273727417, \"max\": 450.026273727417}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3283.1673148247696 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=39, train loss <loss>=4.441195805867513\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] Epoch[40] Batch[0] avg_epoch_loss=4.034159\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=4.034158706665039\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[40] Batch[5] avg_epoch_loss=4.083565\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=4.083565394083659\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[40] Batch [5]#011Speed: 6917.55 samples/sec#011loss=4.083565\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[40] Batch[10] avg_epoch_loss=4.119883\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=4.163464593887329\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[40] Batch [10]#011Speed: 4330.34 samples/sec#011loss=4.163465\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] processed a total of 1550 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564962.66996, \"EndTime\": 1680564963.253124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.6706886291504, \"count\": 1, \"min\": 582.6706886291504, \"max\": 582.6706886291504}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2659.338794767528 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=40, train loss <loss>=4.386317014694214\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[41] Batch[0] avg_epoch_loss=5.111784\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=5.111783981323242\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[41] Batch[5] avg_epoch_loss=4.226768\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=4.226767818133037\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[41] Batch [5]#011Speed: 6891.81 samples/sec#011loss=4.226768\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[41] Batch[10] avg_epoch_loss=4.284867\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=4.354585981369018\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] Epoch[41] Batch [10]#011Speed: 3907.61 samples/sec#011loss=4.354586\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] processed a total of 1530 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564963.2532547, \"EndTime\": 1680564963.8269565, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.9776153564453, \"count\": 1, \"min\": 571.9776153564453, \"max\": 571.9776153564453}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2674.2677116003138 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=41, train loss <loss>=4.389565904935201\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[42] Batch[0] avg_epoch_loss=4.476104\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=4.476104259490967\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[42] Batch[5] avg_epoch_loss=4.277164\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=4.277164498964946\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[42] Batch [5]#011Speed: 9441.91 samples/sec#011loss=4.277164\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[42] Batch[10] avg_epoch_loss=4.087145\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=3.8591216564178468\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[42] Batch [10]#011Speed: 4742.99 samples/sec#011loss=3.859122\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] processed a total of 1602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564963.8270512, \"EndTime\": 1680564964.3296268, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 502.1483898162842, \"count\": 1, \"min\": 502.1483898162842, \"max\": 502.1483898162842}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3189.4591280350005 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=42, train loss <loss>=3.9288295599130483\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[43] Batch[0] avg_epoch_loss=4.003045\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=4.003045082092285\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[43] Batch[5] avg_epoch_loss=3.775126\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=3.7751262187957764\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[43] Batch [5]#011Speed: 8182.26 samples/sec#011loss=3.775126\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[43] Batch[10] avg_epoch_loss=4.081573\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=4.449309253692627\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] Epoch[43] Batch [10]#011Speed: 4914.26 samples/sec#011loss=4.449309\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] processed a total of 1550 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564964.3297098, \"EndTime\": 1680564964.7951486, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.96129035949707, \"count\": 1, \"min\": 464.96129035949707, \"max\": 464.96129035949707}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3332.4932862357455 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=43, train loss <loss>=3.90351611834306\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[44] Batch[0] avg_epoch_loss=3.868311\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=3.8683106899261475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[44] Batch[5] avg_epoch_loss=3.704136\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=3.7041364510854087\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[44] Batch [5]#011Speed: 8022.36 samples/sec#011loss=3.704136\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[44] Batch[10] avg_epoch_loss=4.001445\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=4.358214521408081\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[44] Batch [10]#011Speed: 5870.15 samples/sec#011loss=4.358215\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] processed a total of 1497 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564964.7952576, \"EndTime\": 1680564965.241107, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 445.2376365661621, \"count\": 1, \"min\": 445.2376365661621, \"max\": 445.2376365661621}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3360.860089260767 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=44, train loss <loss>=3.9373634656270347\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[45] Batch[0] avg_epoch_loss=4.810305\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=4.810304641723633\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[45] Batch[5] avg_epoch_loss=4.074875\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=4.074874520301819\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[45] Batch [5]#011Speed: 9691.44 samples/sec#011loss=4.074875\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[45] Batch[10] avg_epoch_loss=4.336463\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=4.650370264053345\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[45] Batch [10]#011Speed: 5469.58 samples/sec#011loss=4.650370\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] processed a total of 1498 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564965.2412431, \"EndTime\": 1680564965.6828256, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 441.15138053894043, \"count\": 1, \"min\": 441.15138053894043, \"max\": 441.15138053894043}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3393.8274335852975 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=45, train loss <loss>=4.408182740211487\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[46] Batch[0] avg_epoch_loss=3.489763\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=3.489762783050537\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[46] Batch[5] avg_epoch_loss=3.575026\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=3.5750260750452676\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:05 INFO 140244386916160] Epoch[46] Batch [5]#011Speed: 7955.98 samples/sec#011loss=3.575026\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[46] Batch[10] avg_epoch_loss=3.899231\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=4.288276481628418\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[46] Batch [10]#011Speed: 6090.05 samples/sec#011loss=4.288276\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] processed a total of 1495 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564965.6830134, \"EndTime\": 1680564966.11836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 434.84997749328613, \"count\": 1, \"min\": 434.84997749328613, \"max\": 434.84997749328613}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3436.889433340733 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=46, train loss <loss>=4.072809835275014\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[47] Batch[0] avg_epoch_loss=3.888751\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=3.8887507915496826\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[47] Batch[5] avg_epoch_loss=3.433285\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=3.4332851568857827\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[47] Batch [5]#011Speed: 8731.91 samples/sec#011loss=3.433285\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[47] Batch[10] avg_epoch_loss=4.019958\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=4.723964500427246\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[47] Batch [10]#011Speed: 5859.95 samples/sec#011loss=4.723965\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] processed a total of 1528 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564966.1184459, \"EndTime\": 1680564966.5418918, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 422.9552745819092, \"count\": 1, \"min\": 422.9552745819092, \"max\": 422.9552745819092}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3611.2074395793375 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=47, train loss <loss>=4.105003158251445\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[48] Batch[0] avg_epoch_loss=4.545450\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=4.545450210571289\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[48] Batch[5] avg_epoch_loss=4.151994\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=4.151993592580159\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[48] Batch [5]#011Speed: 9387.40 samples/sec#011loss=4.151994\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[48] Batch[10] avg_epoch_loss=4.118817\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=4.07900562286377\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:06 INFO 140244386916160] Epoch[48] Batch [10]#011Speed: 4554.82 samples/sec#011loss=4.079006\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] processed a total of 1555 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564966.5419786, \"EndTime\": 1680564967.017781, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 475.325345993042, \"count\": 1, \"min\": 475.325345993042, \"max\": 475.325345993042}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3270.570732693107 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=48, train loss <loss>=3.8690102513019857\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[49] Batch[0] avg_epoch_loss=3.713597\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=3.713597297668457\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[49] Batch[5] avg_epoch_loss=3.900186\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=3.9001862605412803\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[49] Batch [5]#011Speed: 8685.49 samples/sec#011loss=3.900186\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[49] Batch[10] avg_epoch_loss=4.159054\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=4.469695377349853\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[49] Batch [10]#011Speed: 4781.22 samples/sec#011loss=4.469695\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] processed a total of 1551 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564967.017859, \"EndTime\": 1680564967.4871447, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 468.83559226989746, \"count\": 1, \"min\": 468.83559226989746, \"max\": 468.83559226989746}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3307.3280802051904 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=49, train loss <loss>=3.929474500509409\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[50] Batch[0] avg_epoch_loss=3.233038\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=3.2330377101898193\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[50] Batch[5] avg_epoch_loss=3.747123\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=3.7471233208974204\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[50] Batch [5]#011Speed: 9773.94 samples/sec#011loss=3.747123\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[50] Batch[10] avg_epoch_loss=3.904172\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=4.0926313400268555\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] Epoch[50] Batch [10]#011Speed: 5318.72 samples/sec#011loss=4.092631\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] processed a total of 1518 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564967.487219, \"EndTime\": 1680564967.9240303, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 436.2766742706299, \"count\": 1, \"min\": 436.2766742706299, \"max\": 436.2766742706299}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3478.276305100653 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=50, train loss <loss>=3.934478004773458\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[51] Batch[0] avg_epoch_loss=3.335183\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=3.3351831436157227\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[51] Batch[5] avg_epoch_loss=4.126679\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=4.126678625742595\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[51] Batch [5]#011Speed: 9279.88 samples/sec#011loss=4.126679\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[51] Batch[10] avg_epoch_loss=4.009303\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=3.868451976776123\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[51] Batch [10]#011Speed: 4958.93 samples/sec#011loss=3.868452\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] processed a total of 1604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564967.924126, \"EndTime\": 1680564968.384609, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 459.99646186828613, \"count\": 1, \"min\": 459.99646186828613, \"max\": 459.99646186828613}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3485.9805670701376 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=51, train loss <loss>=4.658471914438101\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[52] Batch[0] avg_epoch_loss=3.219507\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=3.2195069789886475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[52] Batch[5] avg_epoch_loss=3.723184\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=3.7231842279434204\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[52] Batch [5]#011Speed: 8041.68 samples/sec#011loss=3.723184\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[52] Batch[10] avg_epoch_loss=4.065641\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=4.476588392257691\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] Epoch[52] Batch [10]#011Speed: 5701.65 samples/sec#011loss=4.476588\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] processed a total of 1510 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564968.3846948, \"EndTime\": 1680564968.8342962, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.17798042297363, \"count\": 1, \"min\": 449.17798042297363, \"max\": 449.17798042297363}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3360.597600027592 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=52, train loss <loss>=4.128563046455383\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[53] Batch[0] avg_epoch_loss=4.381569\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=4.381568908691406\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[53] Batch[5] avg_epoch_loss=3.809726\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=3.8097263177235923\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[53] Batch [5]#011Speed: 9014.98 samples/sec#011loss=3.809726\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[53] Batch[10] avg_epoch_loss=3.934745\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=4.08476767539978\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[53] Batch [10]#011Speed: 4987.31 samples/sec#011loss=4.084768\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] processed a total of 1568 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564968.8343952, \"EndTime\": 1680564969.2979047, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 463.1223678588867, \"count\": 1, \"min\": 463.1223678588867, \"max\": 463.1223678588867}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3384.768070718814 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=53, train loss <loss>=3.840788965041821\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[54] Batch[0] avg_epoch_loss=3.719629\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=3.7196285724639893\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[54] Batch[5] avg_epoch_loss=3.678082\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=3.67808202902476\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[54] Batch [5]#011Speed: 9663.74 samples/sec#011loss=3.678082\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[54] Batch[10] avg_epoch_loss=3.955272\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=4.287900924682617\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[54] Batch [10]#011Speed: 5621.01 samples/sec#011loss=4.287901\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] processed a total of 1515 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564969.2979887, \"EndTime\": 1680564969.7265897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 428.20143699645996, \"count\": 1, \"min\": 428.20143699645996, \"max\": 428.20143699645996}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3536.3551954231525 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=54, train loss <loss>=4.183227082093556\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] Epoch[55] Batch[0] avg_epoch_loss=3.014165\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=3.014164924621582\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[55] Batch[5] avg_epoch_loss=3.959310\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=3.9593103726704917\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[55] Batch [5]#011Speed: 8811.19 samples/sec#011loss=3.959310\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[55] Batch[10] avg_epoch_loss=3.993577\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=4.034697818756103\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[55] Batch [10]#011Speed: 5376.91 samples/sec#011loss=4.034698\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] processed a total of 1532 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564969.7267458, \"EndTime\": 1680564970.166984, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 439.67175483703613, \"count\": 1, \"min\": 439.67175483703613, \"max\": 439.67175483703613}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3483.441012146583 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=55, train loss <loss>=4.075846433639526\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[56] Batch[0] avg_epoch_loss=3.401530\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=3.4015300273895264\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[56] Batch[5] avg_epoch_loss=3.824883\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=3.824882666269938\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[56] Batch [5]#011Speed: 7859.70 samples/sec#011loss=3.824883\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[56] Batch[10] avg_epoch_loss=4.210622\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=4.673509407043457\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[56] Batch [10]#011Speed: 5098.56 samples/sec#011loss=4.673509\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] processed a total of 1512 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564970.167057, \"EndTime\": 1680564970.6183217, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.78134536743164, \"count\": 1, \"min\": 450.78134536743164, \"max\": 450.78134536743164}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3353.030437357246 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=56, train loss <loss>=4.5841976801554365\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[57] Batch[0] avg_epoch_loss=5.146135\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=5.146134853363037\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[57] Batch[5] avg_epoch_loss=4.074968\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=4.074967622756958\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:10 INFO 140244386916160] Epoch[57] Batch [5]#011Speed: 8544.65 samples/sec#011loss=4.074968\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[57] Batch[10] avg_epoch_loss=4.055257\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=4.031604242324829\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[57] Batch [10]#011Speed: 5533.84 samples/sec#011loss=4.031604\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] processed a total of 1465 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564970.6184282, \"EndTime\": 1680564971.0810707, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 462.2061252593994, \"count\": 1, \"min\": 462.2061252593994, \"max\": 462.2061252593994}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3168.4113380314734 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=57, train loss <loss>=3.851305216550827\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[58] Batch[0] avg_epoch_loss=3.882513\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=3.8825128078460693\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[58] Batch[5] avg_epoch_loss=3.719478\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=3.719478130340576\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[58] Batch [5]#011Speed: 9624.27 samples/sec#011loss=3.719478\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[58] Batch[10] avg_epoch_loss=3.916951\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=4.153919267654419\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[58] Batch [10]#011Speed: 4383.69 samples/sec#011loss=4.153919\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] processed a total of 1592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564971.0811954, \"EndTime\": 1680564971.5543983, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 472.76878356933594, \"count\": 1, \"min\": 472.76878356933594, \"max\": 472.76878356933594}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3366.2575137111335 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=58, train loss <loss>=4.06364578467149\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[59] Batch[0] avg_epoch_loss=4.212976\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=4.212976455688477\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[59] Batch[5] avg_epoch_loss=3.959573\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=3.9595728715260825\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[59] Batch [5]#011Speed: 9531.90 samples/sec#011loss=3.959573\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[59] Batch[10] avg_epoch_loss=4.149713\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=4.377880096435547\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] Epoch[59] Batch [10]#011Speed: 5345.22 samples/sec#011loss=4.377880\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] processed a total of 1494 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564971.5545087, \"EndTime\": 1680564971.9961772, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 441.15686416625977, \"count\": 1, \"min\": 441.15686416625977, \"max\": 441.15686416625977}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3385.4179717066504 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=59, train loss <loss>=4.548700292905171\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[60] Batch[0] avg_epoch_loss=4.632560\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=4.6325602531433105\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[60] Batch[5] avg_epoch_loss=3.960800\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=3.960799773534139\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[60] Batch [5]#011Speed: 8433.73 samples/sec#011loss=3.960800\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[60] Batch[10] avg_epoch_loss=4.201809\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=4.491021060943604\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[60] Batch [10]#011Speed: 5505.06 samples/sec#011loss=4.491021\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] processed a total of 1488 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564971.9962764, \"EndTime\": 1680564972.4440947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 447.30162620544434, \"count\": 1, \"min\": 447.30162620544434, \"max\": 447.30162620544434}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3325.20528484523 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=60, train loss <loss>=4.159721394379933\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[61] Batch[0] avg_epoch_loss=4.184512\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=4.184512138366699\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[61] Batch[5] avg_epoch_loss=3.963502\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=3.963502049446106\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[61] Batch [5]#011Speed: 9619.24 samples/sec#011loss=3.963502\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[61] Batch[10] avg_epoch_loss=4.269889\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=4.6375528335571286\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] Epoch[61] Batch [10]#011Speed: 5125.17 samples/sec#011loss=4.637553\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] processed a total of 1563 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564972.4442353, \"EndTime\": 1680564972.8912995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 446.3346004486084, \"count\": 1, \"min\": 446.3346004486084, \"max\": 446.3346004486084}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3500.394934978602 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=61, train loss <loss>=5.048119673362145\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[62] Batch[0] avg_epoch_loss=4.001314\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=4.001314163208008\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[62] Batch[5] avg_epoch_loss=3.859200\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=3.8592002391815186\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[62] Batch [5]#011Speed: 8079.05 samples/sec#011loss=3.859200\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[62] Batch[10] avg_epoch_loss=4.093301\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=4.374222898483277\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[62] Batch [10]#011Speed: 4972.68 samples/sec#011loss=4.374223\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] processed a total of 1573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564972.891437, \"EndTime\": 1680564973.3623536, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 470.2770709991455, \"count\": 1, \"min\": 470.2770709991455, \"max\": 470.2770709991455}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3343.7466763433495 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=62, train loss <loss>=4.061094449116633\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[63] Batch[0] avg_epoch_loss=3.755780\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=3.755779981613159\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[63] Batch[5] avg_epoch_loss=3.910894\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=3.910894433657328\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[63] Batch [5]#011Speed: 8506.45 samples/sec#011loss=3.910894\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[63] Batch[10] avg_epoch_loss=3.877456\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=3.837330198287964\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] Epoch[63] Batch [10]#011Speed: 4371.88 samples/sec#011loss=3.837330\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] processed a total of 1535 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564973.3624566, \"EndTime\": 1680564973.8315434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 468.52636337280273, \"count\": 1, \"min\": 468.52636337280273, \"max\": 468.52636337280273}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3275.1630852251164 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=63, train loss <loss>=4.028244495391846\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[64] Batch[0] avg_epoch_loss=3.907844\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=3.907844066619873\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[64] Batch[5] avg_epoch_loss=4.342699\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=4.342698891957601\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[64] Batch [5]#011Speed: 9329.58 samples/sec#011loss=4.342699\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[64] Batch[10] avg_epoch_loss=4.288322\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=4.223070669174194\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[64] Batch [10]#011Speed: 4886.14 samples/sec#011loss=4.223071\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] processed a total of 1610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564973.8316457, \"EndTime\": 1680564974.2842324, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.085018157959, \"count\": 1, \"min\": 452.085018157959, \"max\": 452.085018157959}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3560.2969361587257 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=64, train loss <loss>=4.731667793714083\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[65] Batch[0] avg_epoch_loss=4.333973\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=4.333973407745361\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[65] Batch[5] avg_epoch_loss=3.973066\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=3.9730660120646157\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[65] Batch [5]#011Speed: 8095.87 samples/sec#011loss=3.973066\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[65] Batch[10] avg_epoch_loss=4.153523\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=4.37007246017456\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[65] Batch [10]#011Speed: 5814.62 samples/sec#011loss=4.370072\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] processed a total of 1521 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564974.2843046, \"EndTime\": 1680564974.7188108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 434.0231418609619, \"count\": 1, \"min\": 434.0231418609619, \"max\": 434.0231418609619}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3503.318720833301 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=65, train loss <loss>=4.202662905057271\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] Epoch[66] Batch[0] avg_epoch_loss=5.461695\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=5.461695194244385\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[66] Batch[5] avg_epoch_loss=4.291279\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=4.291278839111328\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[66] Batch [5]#011Speed: 8994.05 samples/sec#011loss=4.291279\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[66] Batch[10] avg_epoch_loss=4.162373\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=4.007685232162475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[66] Batch [10]#011Speed: 5172.94 samples/sec#011loss=4.007685\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] processed a total of 1560 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564974.7189038, \"EndTime\": 1680564975.1852047, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 465.90232849121094, \"count\": 1, \"min\": 465.90232849121094, \"max\": 465.90232849121094}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3347.282696441598 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=66, train loss <loss>=3.9984632913882914\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[67] Batch[0] avg_epoch_loss=3.673326\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=3.673326015472412\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[67] Batch[5] avg_epoch_loss=3.999287\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=3.999286691347758\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[67] Batch [5]#011Speed: 9468.19 samples/sec#011loss=3.999287\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[67] Batch[10] avg_epoch_loss=4.149042\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=4.328749322891236\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[67] Batch [10]#011Speed: 5770.24 samples/sec#011loss=4.328749\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] processed a total of 1559 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564975.185305, \"EndTime\": 1680564975.643313, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 457.6070308685303, \"count\": 1, \"min\": 457.6070308685303, \"max\": 457.6070308685303}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3405.7566460985404 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=67, train loss <loss>=4.906746864318848\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[68] Batch[0] avg_epoch_loss=3.218022\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=3.218022108078003\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[68] Batch[5] avg_epoch_loss=3.729241\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=3.7292408545811973\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:15 INFO 140244386916160] Epoch[68] Batch [5]#011Speed: 7815.37 samples/sec#011loss=3.729241\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[68] Batch[10] avg_epoch_loss=3.755221\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=3.786396598815918\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[68] Batch [10]#011Speed: 5434.39 samples/sec#011loss=3.786397\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] processed a total of 1557 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564975.643413, \"EndTime\": 1680564976.11558, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 471.68564796447754, \"count\": 1, \"min\": 471.68564796447754, \"max\": 471.68564796447754}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3299.918104010211 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #quality_metric: host=algo-1, epoch=68, train loss <loss>=3.5586211543816786\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[69] Batch[0] avg_epoch_loss=4.449083\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=4.44908332824707\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[69] Batch[5] avg_epoch_loss=4.259638\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=4.259637792905171\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[69] Batch [5]#011Speed: 8120.36 samples/sec#011loss=4.259638\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[69] Batch[10] avg_epoch_loss=4.336398\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=4.428510856628418\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[69] Batch [10]#011Speed: 4588.62 samples/sec#011loss=4.428511\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] processed a total of 1565 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564976.1156805, \"EndTime\": 1680564976.5908976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 474.7040271759033, \"count\": 1, \"min\": 474.7040271759033, \"max\": 474.7040271759033}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3295.854156844256 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #quality_metric: host=algo-1, epoch=69, train loss <loss>=4.430673177425678\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[70] Batch[0] avg_epoch_loss=4.444241\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=4.444240570068359\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[70] Batch[5] avg_epoch_loss=4.113306\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=4.1133058071136475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:16 INFO 140244386916160] Epoch[70] Batch [5]#011Speed: 9084.29 samples/sec#011loss=4.113306\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[70] Batch[10] avg_epoch_loss=4.218288\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=4.344266891479492\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[70] Batch [10]#011Speed: 5843.37 samples/sec#011loss=4.344267\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] processed a total of 1494 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564976.590986, \"EndTime\": 1680564977.0481813, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.76398277282715, \"count\": 1, \"min\": 456.76398277282715, \"max\": 456.76398277282715}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3269.873200961609 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=70, train loss <loss>=4.768023689587911\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[71] Batch[0] avg_epoch_loss=4.065665\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=4.065664768218994\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[71] Batch[5] avg_epoch_loss=3.830237\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=3.8302369912465415\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[71] Batch [5]#011Speed: 9690.81 samples/sec#011loss=3.830237\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[71] Batch[10] avg_epoch_loss=4.089740\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=4.401143169403076\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[71] Batch [10]#011Speed: 5340.66 samples/sec#011loss=4.401143\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] processed a total of 1493 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564977.048258, \"EndTime\": 1680564977.485037, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 436.31577491760254, \"count\": 1, \"min\": 436.31577491760254, \"max\": 436.31577491760254}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3420.7289460688985 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=71, train loss <loss>=4.3167528708775835\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[72] Batch[0] avg_epoch_loss=3.338147\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=3.3381471633911133\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[72] Batch[5] avg_epoch_loss=4.119774\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=4.119773626327515\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[72] Batch [5]#011Speed: 9171.76 samples/sec#011loss=4.119774\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[72] Batch[10] avg_epoch_loss=4.153666\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=4.194336175918579\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] Epoch[72] Batch [10]#011Speed: 5677.28 samples/sec#011loss=4.194336\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] processed a total of 1519 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564977.485129, \"EndTime\": 1680564977.9188516, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 433.1836700439453, \"count\": 1, \"min\": 433.1836700439453, \"max\": 433.1836700439453}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3505.4323180048273 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:17 INFO 140244386916160] #quality_metric: host=algo-1, epoch=72, train loss <loss>=4.167689462502797\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[73] Batch[0] avg_epoch_loss=4.534426\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=4.534425735473633\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[73] Batch[5] avg_epoch_loss=3.727861\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=3.727860927581787\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[73] Batch [5]#011Speed: 9278.12 samples/sec#011loss=3.727861\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[73] Batch[10] avg_epoch_loss=4.030682\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=4.394067907333374\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[73] Batch [10]#011Speed: 5555.50 samples/sec#011loss=4.394068\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] processed a total of 1570 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564977.9189463, \"EndTime\": 1680564978.3688025, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.3136405944824, \"count\": 1, \"min\": 449.3136405944824, \"max\": 449.3136405944824}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3493.098884870372 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #quality_metric: host=algo-1, epoch=73, train loss <loss>=3.8979663848876953\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[74] Batch[0] avg_epoch_loss=3.939754\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=3.9397542476654053\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[74] Batch[5] avg_epoch_loss=4.285281\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=4.285280863444011\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[74] Batch [5]#011Speed: 7898.71 samples/sec#011loss=4.285281\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[74] Batch[10] avg_epoch_loss=4.066383\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=3.8037052154541016\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] Epoch[74] Batch [10]#011Speed: 4921.93 samples/sec#011loss=3.803705\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] processed a total of 1538 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564978.3688984, \"EndTime\": 1680564978.8381886, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 468.7495231628418, \"count\": 1, \"min\": 468.7495231628418, \"max\": 468.7495231628418}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3280.005629689289 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:18 INFO 140244386916160] #quality_metric: host=algo-1, epoch=74, train loss <loss>=4.253260575808012\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[75] Batch[0] avg_epoch_loss=4.447765\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=4.447765350341797\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[75] Batch[5] avg_epoch_loss=4.085669\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=4.085669199625651\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[75] Batch [5]#011Speed: 8947.10 samples/sec#011loss=4.085669\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[75] Batch[10] avg_epoch_loss=4.252368\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=4.452407503128052\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[75] Batch [10]#011Speed: 5960.73 samples/sec#011loss=4.452408\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] processed a total of 1496 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564978.83829, \"EndTime\": 1680564979.2673297, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 428.4491539001465, \"count\": 1, \"min\": 428.4491539001465, \"max\": 428.4491539001465}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3490.5731809010535 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=75, train loss <loss>=4.198373635609944\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[76] Batch[0] avg_epoch_loss=3.358187\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=3.358187437057495\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[76] Batch[5] avg_epoch_loss=3.675351\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=3.67535134156545\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[76] Batch [5]#011Speed: 9534.51 samples/sec#011loss=3.675351\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[76] Batch[10] avg_epoch_loss=3.823609\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=4.001518774032593\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[76] Batch [10]#011Speed: 4791.71 samples/sec#011loss=4.001519\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] processed a total of 1539 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564979.2674153, \"EndTime\": 1680564979.7244313, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.53557777404785, \"count\": 1, \"min\": 456.53557777404785, \"max\": 456.53557777404785}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3369.9090656604153 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=76, train loss <loss>=3.645466332252209\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] Epoch[77] Batch[0] avg_epoch_loss=3.492574\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:19 INFO 140244386916160] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=3.4925737380981445\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[77] Batch[5] avg_epoch_loss=4.076636\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=4.0766355991363525\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[77] Batch [5]#011Speed: 8934.45 samples/sec#011loss=4.076636\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[77] Batch[10] avg_epoch_loss=4.100939\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=4.130102920532226\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[77] Batch [10]#011Speed: 5468.98 samples/sec#011loss=4.130103\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] processed a total of 1453 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564979.7245347, \"EndTime\": 1680564980.1694767, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 444.37265396118164, \"count\": 1, \"min\": 444.37265396118164, \"max\": 444.37265396118164}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3268.194770629853 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=77, train loss <loss>=3.917854497830073\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[78] Batch[0] avg_epoch_loss=4.038235\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=4.038234710693359\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[78] Batch[5] avg_epoch_loss=3.979963\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=3.9799625078837075\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[78] Batch [5]#011Speed: 9484.98 samples/sec#011loss=3.979963\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[78] Batch[10] avg_epoch_loss=4.114518\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=4.275983810424805\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[78] Batch [10]#011Speed: 4929.74 samples/sec#011loss=4.275984\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] processed a total of 1610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564980.1696408, \"EndTime\": 1680564980.6197793, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.6157169342041, \"count\": 1, \"min\": 449.6157169342041, \"max\": 449.6157169342041}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3579.664490082388 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=78, train loss <loss>=4.799384080446684\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[79] Batch[0] avg_epoch_loss=4.427277\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=4.427277088165283\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[79] Batch[5] avg_epoch_loss=3.870163\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=3.870163361231486\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:20 INFO 140244386916160] Epoch[79] Batch [5]#011Speed: 9789.05 samples/sec#011loss=3.870163\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[79] Batch[10] avg_epoch_loss=4.205775\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=4.608508825302124\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[79] Batch [10]#011Speed: 5513.75 samples/sec#011loss=4.608509\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] processed a total of 1548 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564980.6198795, \"EndTime\": 1680564981.0664039, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 446.00796699523926, \"count\": 1, \"min\": 446.00796699523926, \"max\": 446.00796699523926}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3469.6994155340644 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=79, train loss <loss>=3.8336329597693224\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[80] Batch[0] avg_epoch_loss=4.837777\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=4.837777137756348\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[80] Batch[5] avg_epoch_loss=4.187526\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=4.187525749206543\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[80] Batch [5]#011Speed: 9566.82 samples/sec#011loss=4.187526\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[80] Batch[10] avg_epoch_loss=4.058322\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=3.9032783985137938\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[80] Batch [10]#011Speed: 5868.72 samples/sec#011loss=3.903278\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] processed a total of 1502 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564981.066491, \"EndTime\": 1680564981.4965734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 429.553747177124, \"count\": 1, \"min\": 429.553747177124, \"max\": 429.553747177124}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3495.534543114043 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=80, train loss <loss>=4.174046218395233\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[81] Batch[0] avg_epoch_loss=3.902341\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=3.902340888977051\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[81] Batch[5] avg_epoch_loss=4.057569\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=4.0575688282648725\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[81] Batch [5]#011Speed: 8471.20 samples/sec#011loss=4.057569\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[81] Batch[10] avg_epoch_loss=4.159430\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=4.281662511825561\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] Epoch[81] Batch [10]#011Speed: 5272.25 samples/sec#011loss=4.281663\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] processed a total of 1494 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564981.4966605, \"EndTime\": 1680564981.9540591, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.88366889953613, \"count\": 1, \"min\": 456.88366889953613, \"max\": 456.88366889953613}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3268.350199033623 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:21 INFO 140244386916160] #quality_metric: host=algo-1, epoch=81, train loss <loss>=4.111875613530477\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[82] Batch[0] avg_epoch_loss=3.887357\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=3.887357473373413\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[82] Batch[5] avg_epoch_loss=4.104462\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=4.104461709658305\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[82] Batch [5]#011Speed: 9069.44 samples/sec#011loss=4.104462\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[82] Batch[10] avg_epoch_loss=4.072567\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=4.034292793273925\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[82] Batch [10]#011Speed: 5054.40 samples/sec#011loss=4.034293\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] processed a total of 1586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564981.9542375, \"EndTime\": 1680564982.4144034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 459.7623348236084, \"count\": 1, \"min\": 459.7623348236084, \"max\": 459.7623348236084}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3447.979490943319 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #quality_metric: host=algo-1, epoch=82, train loss <loss>=3.990982074003953\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[83] Batch[0] avg_epoch_loss=4.905097\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=4.905096530914307\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[83] Batch[5] avg_epoch_loss=3.761398\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=3.76139763991038\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[83] Batch [5]#011Speed: 9578.87 samples/sec#011loss=3.761398\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[83] Batch[10] avg_epoch_loss=3.762896\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=3.764694833755493\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] Epoch[83] Batch [10]#011Speed: 5123.14 samples/sec#011loss=3.764695\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] processed a total of 1540 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564982.4145722, \"EndTime\": 1680564982.8700402, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 454.8659324645996, \"count\": 1, \"min\": 454.8659324645996, \"max\": 454.8659324645996}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3384.6638380910313 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:22 INFO 140244386916160] #quality_metric: host=algo-1, epoch=83, train loss <loss>=3.605562429015453\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[84] Batch[0] avg_epoch_loss=3.009408\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=3.0094075202941895\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[84] Batch[5] avg_epoch_loss=3.572531\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=3.5725313425064087\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[84] Batch [5]#011Speed: 9088.60 samples/sec#011loss=3.572531\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[84] Batch[10] avg_epoch_loss=4.071478\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=4.670214557647705\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[84] Batch [10]#011Speed: 5865.04 samples/sec#011loss=4.670215\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] processed a total of 1504 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564982.8701212, \"EndTime\": 1680564983.3125744, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 442.0440196990967, \"count\": 1, \"min\": 442.0440196990967, \"max\": 442.0440196990967}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3400.8866432796963 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=84, train loss <loss>=4.064821243286133\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[85] Batch[0] avg_epoch_loss=4.543843\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=4.5438432693481445\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[85] Batch[5] avg_epoch_loss=3.889613\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=3.889612833658854\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[85] Batch [5]#011Speed: 8684.93 samples/sec#011loss=3.889613\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[85] Batch[10] avg_epoch_loss=3.965693\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=4.056989908218384\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[85] Batch [10]#011Speed: 5424.45 samples/sec#011loss=4.056990\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] processed a total of 1504 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564983.312718, \"EndTime\": 1680564983.7523587, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 439.0568733215332, \"count\": 1, \"min\": 439.0568733215332, \"max\": 439.0568733215332}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3424.2822007962195 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=85, train loss <loss>=4.037565847237905\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] Epoch[86] Batch[0] avg_epoch_loss=4.447014\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:23 INFO 140244386916160] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=4.447013854980469\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[86] Batch[5] avg_epoch_loss=3.833380\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=3.833379785219828\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[86] Batch [5]#011Speed: 8129.75 samples/sec#011loss=3.833380\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[86] Batch[10] avg_epoch_loss=4.058587\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=4.328834962844849\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[86] Batch [10]#011Speed: 4738.16 samples/sec#011loss=4.328835\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] processed a total of 1561 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564983.7524724, \"EndTime\": 1680564984.230274, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 477.28633880615234, \"count\": 1, \"min\": 477.28633880615234, \"max\": 477.28633880615234}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3269.5575164319107 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #quality_metric: host=algo-1, epoch=86, train loss <loss>=3.8849901167246013\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[87] Batch[0] avg_epoch_loss=3.399883\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=3.3998825550079346\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[87] Batch[5] avg_epoch_loss=3.763892\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=3.7638922532399497\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[87] Batch [5]#011Speed: 8635.36 samples/sec#011loss=3.763892\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[87] Batch[10] avg_epoch_loss=3.938545\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=4.148129177093506\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[87] Batch [10]#011Speed: 4812.22 samples/sec#011loss=4.148129\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] processed a total of 1561 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564984.230373, \"EndTime\": 1680564984.7120872, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 481.12988471984863, \"count\": 1, \"min\": 481.12988471984863, \"max\": 481.12988471984863}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3243.51070526066 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #quality_metric: host=algo-1, epoch=87, train loss <loss>=3.8812960111177883\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] Epoch[88] Batch[0] avg_epoch_loss=4.502882\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:24 INFO 140244386916160] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=4.50288200378418\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[88] Batch[5] avg_epoch_loss=4.093984\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=4.093984047571818\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[88] Batch [5]#011Speed: 8209.11 samples/sec#011loss=4.093984\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[88] Batch[10] avg_epoch_loss=4.066159\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=4.032769346237183\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[88] Batch [10]#011Speed: 5100.29 samples/sec#011loss=4.032769\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] processed a total of 1522 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564984.7121751, \"EndTime\": 1680564985.1652203, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.53610610961914, \"count\": 1, \"min\": 452.53610610961914, \"max\": 452.53610610961914}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3362.2419093957665 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=88, train loss <loss>=4.164597451686859\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[89] Batch[0] avg_epoch_loss=3.783620\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=3.7836196422576904\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[89] Batch[5] avg_epoch_loss=3.438736\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=3.4387361208597818\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[89] Batch [5]#011Speed: 9507.93 samples/sec#011loss=3.438736\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[89] Batch[10] avg_epoch_loss=3.975202\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=4.618960285186768\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[89] Batch [10]#011Speed: 5181.38 samples/sec#011loss=4.618960\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] processed a total of 1579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564985.1653106, \"EndTime\": 1680564985.6706285, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 504.88948822021484, \"count\": 1, \"min\": 504.88948822021484, \"max\": 504.88948822021484}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3126.4087147359987 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=89, train loss <loss>=4.898718943962684\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[90] Batch[0] avg_epoch_loss=4.296942\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=4.296942234039307\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[90] Batch[5] avg_epoch_loss=4.452780\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=4.45278016726176\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:25 INFO 140244386916160] Epoch[90] Batch [5]#011Speed: 8955.34 samples/sec#011loss=4.452780\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[90] Batch[10] avg_epoch_loss=4.208376\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=3.915091705322266\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[90] Batch [10]#011Speed: 4807.94 samples/sec#011loss=3.915092\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] processed a total of 1551 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564985.6707397, \"EndTime\": 1680564986.1448734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 473.40869903564453, \"count\": 1, \"min\": 473.40869903564453, \"max\": 473.40869903564453}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3275.2821991743026 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #quality_metric: host=algo-1, epoch=90, train loss <loss>=3.9887445844136753\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[91] Batch[0] avg_epoch_loss=3.033330\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=3.033329725265503\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[91] Batch[5] avg_epoch_loss=4.031807\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=4.03180726369222\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[91] Batch [5]#011Speed: 8560.73 samples/sec#011loss=4.031807\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[91] Batch[10] avg_epoch_loss=3.987062\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=3.9333685398101808\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[91] Batch [10]#011Speed: 5403.31 samples/sec#011loss=3.933369\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] processed a total of 1550 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564986.144964, \"EndTime\": 1680564986.6454778, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 500.03886222839355, \"count\": 1, \"min\": 500.03886222839355, \"max\": 500.03886222839355}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3098.8858414056685 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #quality_metric: host=algo-1, epoch=91, train loss <loss>=4.303033370238084\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[92] Batch[0] avg_epoch_loss=4.005304\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=4.005303859710693\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[92] Batch[5] avg_epoch_loss=4.158730\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=4.158730228741963\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:26 INFO 140244386916160] Epoch[92] Batch [5]#011Speed: 9650.54 samples/sec#011loss=4.158730\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[92] Batch[10] avg_epoch_loss=4.362101\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=4.606145715713501\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[92] Batch [10]#011Speed: 4780.51 samples/sec#011loss=4.606146\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] processed a total of 1552 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564986.6455712, \"EndTime\": 1680564987.1054509, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 459.38801765441895, \"count\": 1, \"min\": 459.38801765441895, \"max\": 459.38801765441895}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3377.380505022572 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #quality_metric: host=algo-1, epoch=92, train loss <loss>=4.057706571542299\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[93] Batch[0] avg_epoch_loss=4.247645\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=4.247645378112793\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[93] Batch[5] avg_epoch_loss=4.102964\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=4.102964282035828\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[93] Batch [5]#011Speed: 9626.35 samples/sec#011loss=4.102964\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[93] Batch[10] avg_epoch_loss=4.104114\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=4.105493545532227\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[93] Batch [10]#011Speed: 5060.59 samples/sec#011loss=4.105494\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] processed a total of 1543 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564987.105542, \"EndTime\": 1680564987.570903, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.94197845458984, \"count\": 1, \"min\": 464.94197845458984, \"max\": 464.94197845458984}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3317.7358314139083 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #quality_metric: host=algo-1, epoch=93, train loss <loss>=4.062748083701501\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[94] Batch[0] avg_epoch_loss=5.030309\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=5.030309200286865\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[94] Batch[5] avg_epoch_loss=3.997479\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=3.9974793195724487\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:27 INFO 140244386916160] Epoch[94] Batch [5]#011Speed: 9541.93 samples/sec#011loss=3.997479\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[94] Batch[10] avg_epoch_loss=4.282772\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=4.625123596191406\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[94] Batch [10]#011Speed: 4765.50 samples/sec#011loss=4.625124\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] processed a total of 1523 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564987.5709896, \"EndTime\": 1680564988.018561, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 447.0529556274414, \"count\": 1, \"min\": 447.0529556274414, \"max\": 447.0529556274414}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3405.6491319171396 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=94, train loss <loss>=4.456310013930003\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[95] Batch[0] avg_epoch_loss=4.609714\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=4.609714031219482\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[95] Batch[5] avg_epoch_loss=4.460414\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=4.460413654645284\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[95] Batch [5]#011Speed: 7589.78 samples/sec#011loss=4.460414\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[95] Batch[10] avg_epoch_loss=4.232308\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=3.9585822105407713\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[95] Batch [10]#011Speed: 5200.95 samples/sec#011loss=3.958582\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] processed a total of 1569 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564988.018657, \"EndTime\": 1680564988.4910266, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 471.87280654907227, \"count\": 1, \"min\": 471.87280654907227, \"max\": 471.87280654907227}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3324.096273004857 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=95, train loss <loss>=4.068579857165997\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[96] Batch[0] avg_epoch_loss=4.403892\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=4.4038920402526855\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[96] Batch[5] avg_epoch_loss=4.039449\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=4.039448897043864\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[96] Batch [5]#011Speed: 8709.19 samples/sec#011loss=4.039449\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[96] Batch[10] avg_epoch_loss=3.992082\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=3.9352423667907717\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] Epoch[96] Batch [10]#011Speed: 5525.10 samples/sec#011loss=3.935242\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] processed a total of 1534 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564988.4911163, \"EndTime\": 1680564988.9251335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 433.41660499572754, \"count\": 1, \"min\": 433.41660499572754, \"max\": 433.41660499572754}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3537.97308988971 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:28 INFO 140244386916160] #quality_metric: host=algo-1, epoch=96, train loss <loss>=4.140753149986267\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[97] Batch[0] avg_epoch_loss=4.042463\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=4.0424628257751465\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[97] Batch[5] avg_epoch_loss=3.640681\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=3.6406808296839395\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[97] Batch [5]#011Speed: 9022.98 samples/sec#011loss=3.640681\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[97] Batch[10] avg_epoch_loss=3.855772\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=4.113880586624146\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[97] Batch [10]#011Speed: 5065.02 samples/sec#011loss=4.113881\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] processed a total of 1479 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564988.9252534, \"EndTime\": 1680564989.372369, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 446.608304977417, \"count\": 1, \"min\": 446.608304977417, \"max\": 446.608304977417}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3310.6617432249755 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #quality_metric: host=algo-1, epoch=97, train loss <loss>=3.739398201306661\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[98] Batch[0] avg_epoch_loss=2.965090\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=2.9650895595550537\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[98] Batch[5] avg_epoch_loss=3.682126\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=3.6821258862813315\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[98] Batch [5]#011Speed: 9502.64 samples/sec#011loss=3.682126\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[98] Batch[10] avg_epoch_loss=3.772803\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=3.881616449356079\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] Epoch[98] Batch [10]#011Speed: 5723.02 samples/sec#011loss=3.881616\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] processed a total of 1560 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564989.3724513, \"EndTime\": 1680564989.8154511, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 442.5208568572998, \"count\": 1, \"min\": 442.5208568572998, \"max\": 442.5208568572998}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3524.2702011809843 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:29 INFO 140244386916160] #quality_metric: host=algo-1, epoch=98, train loss <loss>=4.930695698811458\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[99] Batch[0] avg_epoch_loss=3.778302\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=3.778301954269409\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[99] Batch[5] avg_epoch_loss=3.388744\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=3.388744274775187\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[99] Batch [5]#011Speed: 7683.24 samples/sec#011loss=3.388744\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[99] Batch[10] avg_epoch_loss=3.665387\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=3.9973572731018066\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[99] Batch [10]#011Speed: 5641.88 samples/sec#011loss=3.997357\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] processed a total of 1555 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564989.815524, \"EndTime\": 1680564990.2724104, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.3901424407959, \"count\": 1, \"min\": 456.3901424407959, \"max\": 456.3901424407959}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3406.094341839679 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=99, train loss <loss>=3.517098633142618\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[100] Batch[0] avg_epoch_loss=4.001200\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=4.001200199127197\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[100] Batch[5] avg_epoch_loss=3.438807\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=3.4388071298599243\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[100] Batch [5]#011Speed: 9688.26 samples/sec#011loss=3.438807\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[100] Batch[10] avg_epoch_loss=3.879519\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=4.408373737335205\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[100] Batch [10]#011Speed: 5606.40 samples/sec#011loss=4.408374\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] processed a total of 1540 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564990.272498, \"EndTime\": 1680564990.713646, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.65117835998535, \"count\": 1, \"min\": 440.65117835998535, \"max\": 440.65117835998535}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3493.776796710914 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #progress_metric: host=algo-1, completed 50.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=100, train loss <loss>=3.838990706663865\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] Epoch[101] Batch[0] avg_epoch_loss=4.584068\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:30 INFO 140244386916160] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=4.5840678215026855\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[101] Batch[5] avg_epoch_loss=4.043539\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=4.04353928565979\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[101] Batch [5]#011Speed: 8385.73 samples/sec#011loss=4.043539\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[101] Batch[10] avg_epoch_loss=4.118352\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=4.208126592636108\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[101] Batch [10]#011Speed: 5654.68 samples/sec#011loss=4.208127\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] processed a total of 1525 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564990.7137277, \"EndTime\": 1680564991.1632867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.07426834106445, \"count\": 1, \"min\": 449.07426834106445, \"max\": 449.07426834106445}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3394.679912133263 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=101, train loss <loss>=4.171088755130768\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[102] Batch[0] avg_epoch_loss=4.576171\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=4.576170921325684\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[102] Batch[5] avg_epoch_loss=4.195701\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=4.195701082547505\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[102] Batch [5]#011Speed: 9522.60 samples/sec#011loss=4.195701\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[102] Batch[10] avg_epoch_loss=4.060539\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=3.8983437538146974\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[102] Batch [10]#011Speed: 5549.02 samples/sec#011loss=3.898344\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] processed a total of 1554 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564991.1633825, \"EndTime\": 1680564991.6098742, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 445.86706161499023, \"count\": 1, \"min\": 445.86706161499023, \"max\": 445.86706161499023}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3484.1458435008512 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #progress_metric: host=algo-1, completed 51.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=102, train loss <loss>=3.8439212945791392\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[103] Batch[0] avg_epoch_loss=4.852848\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=4.852847576141357\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[103] Batch[5] avg_epoch_loss=3.762725\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=3.7627247174580893\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:31 INFO 140244386916160] Epoch[103] Batch [5]#011Speed: 8384.26 samples/sec#011loss=3.762725\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[103] Batch[10] avg_epoch_loss=4.177176\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=4.674516582489014\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[103] Batch [10]#011Speed: 5397.58 samples/sec#011loss=4.674517\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] processed a total of 1473 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564991.6099768, \"EndTime\": 1680564992.0536928, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 443.21131706237793, \"count\": 1, \"min\": 443.21131706237793, \"max\": 443.21131706237793}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3321.7949729582383 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=103, train loss <loss>=4.863856593767802\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[104] Batch[0] avg_epoch_loss=3.992945\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=3.9929447174072266\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[104] Batch[5] avg_epoch_loss=3.748877\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=3.7488773663838706\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[104] Batch [5]#011Speed: 8795.25 samples/sec#011loss=3.748877\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[104] Batch[10] avg_epoch_loss=4.023960\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=4.354060220718384\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[104] Batch [10]#011Speed: 5359.24 samples/sec#011loss=4.354060\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] processed a total of 1475 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564992.0538628, \"EndTime\": 1680564992.4945471, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.01078605651855, \"count\": 1, \"min\": 440.01078605651855, \"max\": 440.01078605651855}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3351.170060576317 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=104, train loss <loss>=4.090198457241058\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[105] Batch[0] avg_epoch_loss=3.671392\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=3.6713919639587402\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[105] Batch[5] avg_epoch_loss=3.873570\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=3.873569925626119\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[105] Batch [5]#011Speed: 9770.99 samples/sec#011loss=3.873570\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[105] Batch[10] avg_epoch_loss=4.027051\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=4.211229228973389\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] Epoch[105] Batch [10]#011Speed: 5005.40 samples/sec#011loss=4.211229\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] processed a total of 1570 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564992.494634, \"EndTime\": 1680564992.9575632, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 462.51511573791504, \"count\": 1, \"min\": 462.51511573791504, \"max\": 462.51511573791504}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3393.462079357366 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:32 INFO 140244386916160] #quality_metric: host=algo-1, epoch=105, train loss <loss>=4.979513076635508\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[106] Batch[0] avg_epoch_loss=3.259853\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=3.2598533630371094\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[106] Batch[5] avg_epoch_loss=3.976692\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=3.9766921599706015\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[106] Batch [5]#011Speed: 8773.75 samples/sec#011loss=3.976692\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[106] Batch[10] avg_epoch_loss=3.972973\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=3.968509006500244\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[106] Batch [10]#011Speed: 4644.45 samples/sec#011loss=3.968509\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] processed a total of 1557 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564992.9576545, \"EndTime\": 1680564993.4319038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 473.71912002563477, \"count\": 1, \"min\": 473.71912002563477, \"max\": 473.71912002563477}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3285.6895099941235 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #progress_metric: host=algo-1, completed 53.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #quality_metric: host=algo-1, epoch=106, train loss <loss>=3.7867041642849264\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[107] Batch[0] avg_epoch_loss=3.745472\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=3.745471715927124\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[107] Batch[5] avg_epoch_loss=4.135463\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=4.135462681452434\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[107] Batch [5]#011Speed: 7775.19 samples/sec#011loss=4.135463\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[107] Batch[10] avg_epoch_loss=4.204744\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=4.287881183624267\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] Epoch[107] Batch [10]#011Speed: 4666.97 samples/sec#011loss=4.287881\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] processed a total of 1566 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564993.4320014, \"EndTime\": 1680564993.917329, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 484.73501205444336, \"count\": 1, \"min\": 484.73501205444336, \"max\": 484.73501205444336}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3229.5795156623994 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:33 INFO 140244386916160] #quality_metric: host=algo-1, epoch=107, train loss <loss>=3.9966879166089573\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[108] Batch[0] avg_epoch_loss=4.063833\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=4.063832759857178\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[108] Batch[5] avg_epoch_loss=3.503587\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=3.503587245941162\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[108] Batch [5]#011Speed: 8016.73 samples/sec#011loss=3.503587\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[108] Batch[10] avg_epoch_loss=3.757577\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=4.062365531921387\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[108] Batch [10]#011Speed: 5579.25 samples/sec#011loss=4.062366\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] processed a total of 1508 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564993.917436, \"EndTime\": 1680564994.3610055, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 443.0379867553711, \"count\": 1, \"min\": 443.0379867553711, \"max\": 443.0379867553711}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3402.6873018912993 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #progress_metric: host=algo-1, completed 54.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #quality_metric: host=algo-1, epoch=108, train loss <loss>=3.858038902282715\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[109] Batch[0] avg_epoch_loss=3.910931\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=3.91093111038208\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[109] Batch[5] avg_epoch_loss=3.546111\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=3.5461106300354004\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[109] Batch [5]#011Speed: 8550.72 samples/sec#011loss=3.546111\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[109] Batch[10] avg_epoch_loss=3.632149\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=3.7353954792022703\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] Epoch[109] Batch [10]#011Speed: 4665.12 samples/sec#011loss=3.735395\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] processed a total of 1555 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564994.3610978, \"EndTime\": 1680564994.8259497, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.31446075439453, \"count\": 1, \"min\": 464.31446075439453, \"max\": 464.31446075439453}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3347.9644287846777 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:34 INFO 140244386916160] #quality_metric: host=algo-1, epoch=109, train loss <loss>=3.469585372851445\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[110] Batch[0] avg_epoch_loss=3.970313\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=3.9703128337860107\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[110] Batch[5] avg_epoch_loss=3.819481\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=3.8194806575775146\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[110] Batch [5]#011Speed: 9292.25 samples/sec#011loss=3.819481\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[110] Batch[10] avg_epoch_loss=3.988879\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=4.192158031463623\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[110] Batch [10]#011Speed: 4996.28 samples/sec#011loss=4.192158\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] processed a total of 1552 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564994.8260484, \"EndTime\": 1680564995.3055422, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 479.0973663330078, \"count\": 1, \"min\": 479.0973663330078, \"max\": 479.0973663330078}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3238.495131232597 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #progress_metric: host=algo-1, completed 55.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #quality_metric: host=algo-1, epoch=110, train loss <loss>=4.033640347994291\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[111] Batch[0] avg_epoch_loss=4.329439\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=4.32943868637085\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[111] Batch[5] avg_epoch_loss=4.271871\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=4.271870891253154\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[111] Batch [5]#011Speed: 9081.77 samples/sec#011loss=4.271871\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[111] Batch[10] avg_epoch_loss=4.164955\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=4.036656332015991\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] Epoch[111] Batch [10]#011Speed: 5156.66 samples/sec#011loss=4.036656\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] processed a total of 1553 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564995.3056338, \"EndTime\": 1680564995.7958276, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 489.760160446167, \"count\": 1, \"min\": 489.760160446167, \"max\": 489.760160446167}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3170.128085990902 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:35 INFO 140244386916160] #quality_metric: host=algo-1, epoch=111, train loss <loss>=4.163139581680298\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[112] Batch[0] avg_epoch_loss=3.881649\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=3.8816490173339844\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[112] Batch[5] avg_epoch_loss=4.026449\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=4.0264493227005005\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[112] Batch [5]#011Speed: 9658.11 samples/sec#011loss=4.026449\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[112] Batch[10] avg_epoch_loss=4.097327\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=4.182381010055542\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[112] Batch [10]#011Speed: 4993.79 samples/sec#011loss=4.182381\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] processed a total of 1557 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564995.7959044, \"EndTime\": 1680564996.2498536, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 453.4726142883301, \"count\": 1, \"min\": 453.4726142883301, \"max\": 453.4726142883301}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3432.417533023162 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #progress_metric: host=algo-1, completed 56.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=112, train loss <loss>=4.774973135728103\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[113] Batch[0] avg_epoch_loss=3.407107\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=3.407107353210449\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[113] Batch[5] avg_epoch_loss=3.772885\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=3.7728846073150635\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[113] Batch [5]#011Speed: 9297.05 samples/sec#011loss=3.772885\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[113] Batch[10] avg_epoch_loss=3.999555\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=4.271560001373291\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[113] Batch [10]#011Speed: 5693.90 samples/sec#011loss=4.271560\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] processed a total of 1492 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564996.2499473, \"EndTime\": 1680564996.682347, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 431.9477081298828, \"count\": 1, \"min\": 431.9477081298828, \"max\": 431.9477081298828}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3453.0468557908143 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=113, train loss <loss>=3.961966872215271\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[114] Batch[0] avg_epoch_loss=4.965458\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=4.965458393096924\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[114] Batch[5] avg_epoch_loss=4.005908\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=4.005907853444417\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:36 INFO 140244386916160] Epoch[114] Batch [5]#011Speed: 9501.16 samples/sec#011loss=4.005908\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[114] Batch[10] avg_epoch_loss=4.048297\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=4.09916410446167\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[114] Batch [10]#011Speed: 4883.18 samples/sec#011loss=4.099164\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] processed a total of 1577 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564996.6824307, \"EndTime\": 1680564997.1402388, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 457.3354721069336, \"count\": 1, \"min\": 457.3354721069336, \"max\": 457.3354721069336}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3447.253372247557 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=114, train loss <loss>=3.9460908266214223\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[115] Batch[0] avg_epoch_loss=2.393597\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=2.393596887588501\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[115] Batch[5] avg_epoch_loss=3.661830\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=3.661830027898153\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[115] Batch [5]#011Speed: 8956.66 samples/sec#011loss=3.661830\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[115] Batch[10] avg_epoch_loss=3.717889\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=3.7851593494415283\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[115] Batch [10]#011Speed: 5905.14 samples/sec#011loss=3.785159\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] processed a total of 1540 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564997.1403253, \"EndTime\": 1680564997.5816255, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.7374858856201, \"count\": 1, \"min\": 440.7374858856201, \"max\": 440.7374858856201}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3492.7037352062807 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=115, train loss <loss>=4.723768032514132\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[116] Batch[0] avg_epoch_loss=3.737267\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=3.737266778945923\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[116] Batch[5] avg_epoch_loss=4.063262\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=4.0632619460423784\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[116] Batch [5]#011Speed: 9183.65 samples/sec#011loss=4.063262\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[116] Batch[10] avg_epoch_loss=3.949598\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=3.8132014751434324\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:37 INFO 140244386916160] Epoch[116] Batch [10]#011Speed: 5643.76 samples/sec#011loss=3.813201\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] processed a total of 1560 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564997.5817542, \"EndTime\": 1680564998.0247374, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 442.46935844421387, \"count\": 1, \"min\": 442.46935844421387, \"max\": 442.46935844421387}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3524.4961076625077 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #progress_metric: host=algo-1, completed 58.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=116, train loss <loss>=4.121321017925556\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[117] Batch[0] avg_epoch_loss=4.849705\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=4.849705219268799\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[117] Batch[5] avg_epoch_loss=3.636791\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=3.636791149775187\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[117] Batch [5]#011Speed: 8651.00 samples/sec#011loss=3.636791\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[117] Batch[10] avg_epoch_loss=3.838731\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=4.081059503555298\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[117] Batch [10]#011Speed: 5687.32 samples/sec#011loss=4.081060\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] processed a total of 1534 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564998.0248358, \"EndTime\": 1680564998.4637265, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 438.34495544433594, \"count\": 1, \"min\": 438.34495544433594, \"max\": 438.34495544433594}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3497.7957337525863 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=117, train loss <loss>=3.920601546764374\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[118] Batch[0] avg_epoch_loss=3.463437\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=3.46343731880188\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[118] Batch[5] avg_epoch_loss=3.823061\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=3.823061386744181\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[118] Batch [5]#011Speed: 9739.90 samples/sec#011loss=3.823061\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[118] Batch[10] avg_epoch_loss=3.945556\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=4.092549610137939\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] Epoch[118] Batch [10]#011Speed: 5495.82 samples/sec#011loss=4.092550\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] processed a total of 1527 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564998.4638848, \"EndTime\": 1680564998.8955748, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 431.1404228210449, \"count\": 1, \"min\": 431.1404228210449, \"max\": 431.1404228210449}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3540.586912320931 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #progress_metric: host=algo-1, completed 59.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:38 INFO 140244386916160] #quality_metric: host=algo-1, epoch=118, train loss <loss>=4.031205058097839\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[119] Batch[0] avg_epoch_loss=3.829790\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=3.829789638519287\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[119] Batch[5] avg_epoch_loss=3.776982\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=3.7769820292790732\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[119] Batch [5]#011Speed: 9503.99 samples/sec#011loss=3.776982\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[119] Batch[10] avg_epoch_loss=3.896487\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=4.039893388748169\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[119] Batch [10]#011Speed: 5047.36 samples/sec#011loss=4.039893\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] processed a total of 1523 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564998.8956654, \"EndTime\": 1680564999.3392186, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 443.07589530944824, \"count\": 1, \"min\": 443.07589530944824, \"max\": 443.07589530944824}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3434.2482612166777 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #quality_metric: host=algo-1, epoch=119, train loss <loss>=3.9100388288497925\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[120] Batch[0] avg_epoch_loss=2.867657\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=2.867656946182251\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[120] Batch[5] avg_epoch_loss=3.987673\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=3.9876728852589927\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[120] Batch [5]#011Speed: 9080.58 samples/sec#011loss=3.987673\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[120] Batch[10] avg_epoch_loss=4.150008\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=4.3448103904724125\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] Epoch[120] Batch [10]#011Speed: 5748.60 samples/sec#011loss=4.344810\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] processed a total of 1522 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564999.3395689, \"EndTime\": 1680564999.8011074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 461.1246585845947, \"count\": 1, \"min\": 461.1246585845947, \"max\": 461.1246585845947}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3299.3533263422405 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #progress_metric: host=algo-1, completed 60.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:39 INFO 140244386916160] #quality_metric: host=algo-1, epoch=120, train loss <loss>=4.108123222986857\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[121] Batch[0] avg_epoch_loss=3.549633\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=3.5496327877044678\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[121] Batch[5] avg_epoch_loss=4.060594\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=4.060594479242961\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[121] Batch [5]#011Speed: 9267.39 samples/sec#011loss=4.060594\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[121] Batch[10] avg_epoch_loss=3.990274\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=3.905890130996704\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[121] Batch [10]#011Speed: 5317.19 samples/sec#011loss=3.905890\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] processed a total of 1550 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680564999.8012383, \"EndTime\": 1680565000.249534, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 447.86787033081055, \"count\": 1, \"min\": 447.86787033081055, \"max\": 447.86787033081055}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3459.7499743226363 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=121, train loss <loss>=4.229372923190777\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[122] Batch[0] avg_epoch_loss=4.261995\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=4.261995315551758\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[122] Batch[5] avg_epoch_loss=3.997662\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=3.9976621866226196\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[122] Batch [5]#011Speed: 9385.49 samples/sec#011loss=3.997662\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[122] Batch[10] avg_epoch_loss=3.843868\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=3.6593153953552244\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[122] Batch [10]#011Speed: 5217.01 samples/sec#011loss=3.659315\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] processed a total of 1593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565000.2496293, \"EndTime\": 1680565000.7002013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.1080513000488, \"count\": 1, \"min\": 450.1080513000488, \"max\": 450.1080513000488}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3538.0126481535317 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #progress_metric: host=algo-1, completed 61.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=122, train loss <loss>=4.056781200262217\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] Epoch[123] Batch[0] avg_epoch_loss=3.077587\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:40 INFO 140244386916160] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=3.0775868892669678\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[123] Batch[5] avg_epoch_loss=3.649453\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=3.6494526068369546\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[123] Batch [5]#011Speed: 8527.17 samples/sec#011loss=3.649453\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[123] Batch[10] avg_epoch_loss=3.823130\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=4.0315436840057375\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[123] Batch [10]#011Speed: 5675.71 samples/sec#011loss=4.031544\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] processed a total of 1497 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565000.7002966, \"EndTime\": 1680565001.1514845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.5422115325928, \"count\": 1, \"min\": 450.5422115325928, \"max\": 450.5422115325928}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3320.830255019955 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=123, train loss <loss>=3.7837488055229187\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[124] Batch[0] avg_epoch_loss=3.230863\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=3.230863094329834\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[124] Batch[5] avg_epoch_loss=3.967151\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=3.9671512047449746\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[124] Batch [5]#011Speed: 9516.93 samples/sec#011loss=3.967151\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[124] Batch[10] avg_epoch_loss=4.085349\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=4.227186775207519\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[124] Batch [10]#011Speed: 5257.15 samples/sec#011loss=4.227187\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] processed a total of 1598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565001.1516254, \"EndTime\": 1680565001.5948546, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 442.54112243652344, \"count\": 1, \"min\": 442.54112243652344, \"max\": 442.54112243652344}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3609.783387073928 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #progress_metric: host=algo-1, completed 62.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=124, train loss <loss>=3.9608138707967906\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[125] Batch[0] avg_epoch_loss=4.439837\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=4.439836502075195\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[125] Batch[5] avg_epoch_loss=4.159394\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=4.159394105275472\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:41 INFO 140244386916160] Epoch[125] Batch [5]#011Speed: 9529.56 samples/sec#011loss=4.159394\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[125] Batch[10] avg_epoch_loss=4.179087\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=4.20271863937378\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[125] Batch [10]#011Speed: 5607.03 samples/sec#011loss=4.202719\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] processed a total of 1461 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565001.594948, \"EndTime\": 1680565002.0399055, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 444.4425106048584, \"count\": 1, \"min\": 444.4425106048584, \"max\": 444.4425106048584}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3285.8805906123184 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=125, train loss <loss>=4.021888116995494\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[126] Batch[0] avg_epoch_loss=4.592618\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=4.592618465423584\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[126] Batch[5] avg_epoch_loss=3.414455\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=3.414455016454061\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[126] Batch [5]#011Speed: 9195.42 samples/sec#011loss=3.414455\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[126] Batch[10] avg_epoch_loss=3.640159\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=3.911003017425537\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[126] Batch [10]#011Speed: 4653.29 samples/sec#011loss=3.911003\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] processed a total of 1557 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565002.040041, \"EndTime\": 1680565002.4993603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 458.77814292907715, \"count\": 1, \"min\": 458.77814292907715, \"max\": 458.77814292907715}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3392.78519182743 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #progress_metric: host=algo-1, completed 63.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=126, train loss <loss>=4.814652076134315\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[127] Batch[0] avg_epoch_loss=2.689857\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=2.6898574829101562\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[127] Batch[5] avg_epoch_loss=3.506640\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=3.5066397190093994\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[127] Batch [5]#011Speed: 9640.90 samples/sec#011loss=3.506640\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[127] Batch[10] avg_epoch_loss=3.830406\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=4.218925285339355\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] Epoch[127] Batch [10]#011Speed: 5603.93 samples/sec#011loss=4.218925\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] processed a total of 1539 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565002.4994507, \"EndTime\": 1680565002.962656, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 462.8167152404785, \"count\": 1, \"min\": 462.8167152404785, \"max\": 462.8167152404785}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3324.3211601845746 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:42 INFO 140244386916160] #quality_metric: host=algo-1, epoch=127, train loss <loss>=4.064030427199143\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[128] Batch[0] avg_epoch_loss=4.319608\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=4.319607734680176\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[128] Batch[5] avg_epoch_loss=3.978450\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=3.978449900945028\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[128] Batch [5]#011Speed: 8477.01 samples/sec#011loss=3.978450\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[128] Batch[10] avg_epoch_loss=3.982682\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=3.9877607822418213\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[128] Batch [10]#011Speed: 5108.17 samples/sec#011loss=3.987761\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] processed a total of 1551 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565002.9627457, \"EndTime\": 1680565003.418684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 455.4293155670166, \"count\": 1, \"min\": 455.4293155670166, \"max\": 455.4293155670166}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3404.4746591529956 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #progress_metric: host=algo-1, completed 64.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #quality_metric: host=algo-1, epoch=128, train loss <loss>=3.796219669855558\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[129] Batch[0] avg_epoch_loss=3.313822\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=3.3138222694396973\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[129] Batch[5] avg_epoch_loss=3.770927\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=3.7709269920984902\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[129] Batch [5]#011Speed: 9299.82 samples/sec#011loss=3.770927\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[129] Batch[10] avg_epoch_loss=4.110452\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=4.5178814888000485\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] Epoch[129] Batch [10]#011Speed: 5871.69 samples/sec#011loss=4.517881\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] processed a total of 1484 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565003.4187815, \"EndTime\": 1680565003.8471286, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.86478996276855, \"count\": 1, \"min\": 427.86478996276855, \"max\": 427.86478996276855}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3467.168258017797 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:43 INFO 140244386916160] #quality_metric: host=algo-1, epoch=129, train loss <loss>=4.005987723668416\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[130] Batch[0] avg_epoch_loss=3.345397\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=3.3453967571258545\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[130] Batch[5] avg_epoch_loss=3.892227\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=3.8922268946965537\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[130] Batch [5]#011Speed: 8507.42 samples/sec#011loss=3.892227\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[130] Batch[10] avg_epoch_loss=3.861588\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=3.8248221397399904\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[130] Batch [10]#011Speed: 5628.00 samples/sec#011loss=3.824822\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] processed a total of 1556 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565003.8472216, \"EndTime\": 1680565004.3112824, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 463.590145111084, \"count\": 1, \"min\": 463.590145111084, \"max\": 463.590145111084}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3354.9860939255063 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #progress_metric: host=algo-1, completed 65.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=130, train loss <loss>=3.6419702676626353\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[131] Batch[0] avg_epoch_loss=3.094597\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=3.0945968627929688\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[131] Batch[5] avg_epoch_loss=3.314323\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=3.3143227895100913\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[131] Batch [5]#011Speed: 8348.85 samples/sec#011loss=3.314323\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[131] Batch[10] avg_epoch_loss=3.671358\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=4.0998005867004395\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] Epoch[131] Batch [10]#011Speed: 5024.30 samples/sec#011loss=4.099801\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] processed a total of 1552 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565004.3114321, \"EndTime\": 1680565004.7768419, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.874267578125, \"count\": 1, \"min\": 464.874267578125, \"max\": 464.874267578125}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3337.6127779224803 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:44 INFO 140244386916160] #quality_metric: host=algo-1, epoch=131, train loss <loss>=3.736002408541166\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[132] Batch[0] avg_epoch_loss=3.333050\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=3.3330495357513428\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[132] Batch[5] avg_epoch_loss=3.636439\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=3.6364392836888633\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[132] Batch [5]#011Speed: 8933.79 samples/sec#011loss=3.636439\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[132] Batch[10] avg_epoch_loss=3.974542\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=4.3802652835845945\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[132] Batch [10]#011Speed: 6216.06 samples/sec#011loss=4.380265\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] processed a total of 1479 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565004.7769237, \"EndTime\": 1680565005.198262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 420.839786529541, \"count\": 1, \"min\": 420.839786529541, \"max\": 420.839786529541}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3513.2391859610334 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #progress_metric: host=algo-1, completed 66.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=132, train loss <loss>=3.846078375975291\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[133] Batch[0] avg_epoch_loss=4.357931\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=4.357930660247803\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[133] Batch[5] avg_epoch_loss=3.688226\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=3.6882256666819253\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[133] Batch [5]#011Speed: 9617.59 samples/sec#011loss=3.688226\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[133] Batch[10] avg_epoch_loss=3.846041\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=4.0354187965393065\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[133] Batch [10]#011Speed: 4739.08 samples/sec#011loss=4.035419\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] processed a total of 1572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565005.1983504, \"EndTime\": 1680565005.6549642, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.1586380004883, \"count\": 1, \"min\": 456.1586380004883, \"max\": 456.1586380004883}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3445.1719585206984 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=133, train loss <loss>=3.7361739736336927\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[134] Batch[0] avg_epoch_loss=3.371536\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=3.3715357780456543\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[134] Batch[5] avg_epoch_loss=3.744709\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=3.7447089354197183\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:45 INFO 140244386916160] Epoch[134] Batch [5]#011Speed: 9139.07 samples/sec#011loss=3.744709\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[134] Batch[10] avg_epoch_loss=4.066070\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=4.4517041683197025\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[134] Batch [10]#011Speed: 5633.68 samples/sec#011loss=4.451704\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] processed a total of 1461 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565005.6550484, \"EndTime\": 1680565006.1019702, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 446.4237689971924, \"count\": 1, \"min\": 446.4237689971924, \"max\": 446.4237689971924}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3271.4906153653137 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #progress_metric: host=algo-1, completed 67.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=134, train loss <loss>=3.871536542971929\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[135] Batch[0] avg_epoch_loss=4.644758\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=4.6447577476501465\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[135] Batch[5] avg_epoch_loss=4.278659\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=4.278659383455913\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[135] Batch [5]#011Speed: 8310.13 samples/sec#011loss=4.278659\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[135] Batch[10] avg_epoch_loss=4.140075\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=3.9737740039825438\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[135] Batch [10]#011Speed: 5340.84 samples/sec#011loss=3.973774\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] processed a total of 1562 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565006.1020734, \"EndTime\": 1680565006.570479, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 467.97823905944824, \"count\": 1, \"min\": 467.97823905944824, \"max\": 467.97823905944824}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3336.8100425638545 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=135, train loss <loss>=3.9065145620932946\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[136] Batch[0] avg_epoch_loss=4.881260\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=4.881259918212891\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[136] Batch[5] avg_epoch_loss=4.134258\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=4.134258270263672\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:46 INFO 140244386916160] Epoch[136] Batch [5]#011Speed: 9347.68 samples/sec#011loss=4.134258\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[136] Batch[10] avg_epoch_loss=4.040551\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=3.92810320854187\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[136] Batch [10]#011Speed: 4776.50 samples/sec#011loss=3.928103\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] processed a total of 1584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565006.570561, \"EndTime\": 1680565007.0428083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 471.7447757720947, \"count\": 1, \"min\": 471.7447757720947, \"max\": 471.7447757720947}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3356.6790682023834 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #progress_metric: host=algo-1, completed 68.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=136, train loss <loss>=3.8971882049853983\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[137] Batch[0] avg_epoch_loss=3.869165\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=3.8691651821136475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[137] Batch[5] avg_epoch_loss=3.534168\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=3.5341676076253257\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[137] Batch [5]#011Speed: 8969.02 samples/sec#011loss=3.534168\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[137] Batch[10] avg_epoch_loss=3.731450\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=3.9681893825531005\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[137] Batch [10]#011Speed: 5582.19 samples/sec#011loss=3.968189\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] processed a total of 1512 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565007.042902, \"EndTime\": 1680565007.4831913, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 439.77999687194824, \"count\": 1, \"min\": 439.77999687194824, \"max\": 439.77999687194824}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3436.9107132018207 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=137, train loss <loss>=3.667881409327189\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[138] Batch[0] avg_epoch_loss=2.958395\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=2.958395481109619\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[138] Batch[5] avg_epoch_loss=3.821972\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=3.8219720125198364\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[138] Batch [5]#011Speed: 8511.20 samples/sec#011loss=3.821972\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[138] Batch[10] avg_epoch_loss=3.872430\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=3.932979869842529\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] Epoch[138] Batch [10]#011Speed: 5399.97 samples/sec#011loss=3.932980\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] processed a total of 1564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565007.4832973, \"EndTime\": 1680565007.932138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 448.3308792114258, \"count\": 1, \"min\": 448.3308792114258, \"max\": 448.3308792114258}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3487.354386457358 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #progress_metric: host=algo-1, completed 69.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:47 INFO 140244386916160] #quality_metric: host=algo-1, epoch=138, train loss <loss>=4.078377558634831\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[139] Batch[0] avg_epoch_loss=4.918721\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=4.918720722198486\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[139] Batch[5] avg_epoch_loss=3.832015\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=3.8320151567459106\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[139] Batch [5]#011Speed: 8722.52 samples/sec#011loss=3.832015\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[139] Batch[10] avg_epoch_loss=4.015861\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=4.236476993560791\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[139] Batch [10]#011Speed: 5571.73 samples/sec#011loss=4.236477\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] processed a total of 1561 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565007.9322374, \"EndTime\": 1680565008.3848379, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.1505832672119, \"count\": 1, \"min\": 452.1505832672119, \"max\": 452.1505832672119}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3451.42691827984 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=139, train loss <loss>=4.70814193212069\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[140] Batch[0] avg_epoch_loss=3.151938\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=3.151937961578369\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[140] Batch[5] avg_epoch_loss=3.699309\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=3.6993094285329184\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[140] Batch [5]#011Speed: 9403.12 samples/sec#011loss=3.699309\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[140] Batch[10] avg_epoch_loss=3.745995\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=3.802017641067505\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] Epoch[140] Batch [10]#011Speed: 5300.76 samples/sec#011loss=3.802018\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] processed a total of 1493 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565008.3849144, \"EndTime\": 1680565008.8181307, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 432.56664276123047, \"count\": 1, \"min\": 432.56664276123047, \"max\": 432.56664276123047}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3450.283711240689 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #progress_metric: host=algo-1, completed 70.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:48 INFO 140244386916160] #quality_metric: host=algo-1, epoch=140, train loss <loss>=4.29686560233434\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[141] Batch[0] avg_epoch_loss=4.366227\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=4.366226673126221\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[141] Batch[5] avg_epoch_loss=3.483309\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=3.483308712641398\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[141] Batch [5]#011Speed: 8728.64 samples/sec#011loss=3.483309\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[141] Batch[10] avg_epoch_loss=3.698549\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=3.956838083267212\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[141] Batch [10]#011Speed: 5067.64 samples/sec#011loss=3.956838\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] processed a total of 1564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565008.8182378, \"EndTime\": 1680565009.3205009, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 501.8470287322998, \"count\": 1, \"min\": 501.8470287322998, \"max\": 501.8470287322998}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3115.1969782089313 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=141, train loss <loss>=3.5043495343281674\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[142] Batch[0] avg_epoch_loss=4.492849\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=4.492848873138428\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[142] Batch[5] avg_epoch_loss=4.461310\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=4.461309512456258\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[142] Batch [5]#011Speed: 9615.79 samples/sec#011loss=4.461310\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[142] Batch[10] avg_epoch_loss=4.294512\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=4.094355487823487\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] Epoch[142] Batch [10]#011Speed: 4853.04 samples/sec#011loss=4.094355\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] processed a total of 1591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565009.3206563, \"EndTime\": 1680565009.7686594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 447.3917484283447, \"count\": 1, \"min\": 447.3917484283447, \"max\": 447.3917484283447}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3555.0459589289585 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #progress_metric: host=algo-1, completed 71.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:49 INFO 140244386916160] #quality_metric: host=algo-1, epoch=142, train loss <loss>=4.366732927469107\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[143] Batch[0] avg_epoch_loss=3.355436\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=3.355435609817505\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[143] Batch[5] avg_epoch_loss=3.600351\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=3.600350856781006\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[143] Batch [5]#011Speed: 9474.17 samples/sec#011loss=3.600351\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[143] Batch[10] avg_epoch_loss=4.032183\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=4.550380897521973\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[143] Batch [10]#011Speed: 5416.45 samples/sec#011loss=4.550381\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] processed a total of 1514 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565009.768746, \"EndTime\": 1680565010.2296543, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 460.4814052581787, \"count\": 1, \"min\": 460.4814052581787, \"max\": 460.4814052581787}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3286.8407122153208 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=143, train loss <loss>=4.200550397237142\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[144] Batch[0] avg_epoch_loss=4.104948\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=4.104947566986084\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[144] Batch[5] avg_epoch_loss=3.618882\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=3.6188817818959556\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[144] Batch [5]#011Speed: 8572.00 samples/sec#011loss=3.618882\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[144] Batch[10] avg_epoch_loss=3.913772\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=4.2676411151885985\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[144] Batch [10]#011Speed: 5493.71 samples/sec#011loss=4.267641\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] processed a total of 1509 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565010.2297518, \"EndTime\": 1680565010.6711688, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.88244438171387, \"count\": 1, \"min\": 440.88244438171387, \"max\": 440.88244438171387}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3421.5413012464514 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #progress_metric: host=algo-1, completed 72.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=144, train loss <loss>=4.016784250736237\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[145] Batch[0] avg_epoch_loss=4.687806\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=4.687806129455566\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[145] Batch[5] avg_epoch_loss=3.903011\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=3.9030109643936157\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:50 INFO 140244386916160] Epoch[145] Batch [5]#011Speed: 7675.35 samples/sec#011loss=3.903011\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[145] Batch[10] avg_epoch_loss=3.991473\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=4.097628259658814\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[145] Batch [10]#011Speed: 5439.96 samples/sec#011loss=4.097628\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] processed a total of 1503 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565010.67125, \"EndTime\": 1680565011.135725, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 463.95230293273926, \"count\": 1, \"min\": 463.95230293273926, \"max\": 463.95230293273926}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3238.518670427791 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #progress_metric: host=algo-1, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=145, train loss <loss>=4.06089174747467\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[146] Batch[0] avg_epoch_loss=3.484765\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=3.4847652912139893\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[146] Batch[5] avg_epoch_loss=3.778742\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=3.77874227364858\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[146] Batch [5]#011Speed: 9224.11 samples/sec#011loss=3.778742\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[146] Batch[10] avg_epoch_loss=4.104873\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=4.496230173110962\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[146] Batch [10]#011Speed: 5476.81 samples/sec#011loss=4.496230\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] processed a total of 1536 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565011.1358223, \"EndTime\": 1680565011.5635293, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.19388008117676, \"count\": 1, \"min\": 427.19388008117676, \"max\": 427.19388008117676}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3594.3194223604355 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #progress_metric: host=algo-1, completed 73.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=146, train loss <loss>=4.166768511136373\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[147] Batch[0] avg_epoch_loss=3.529987\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=3.52998685836792\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[147] Batch[5] avg_epoch_loss=3.674654\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=3.6746538480122886\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[147] Batch [5]#011Speed: 9479.26 samples/sec#011loss=3.674654\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[147] Batch[10] avg_epoch_loss=3.779790\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=147, batch=10 train loss <loss>=3.9059539318084715\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] Epoch[147] Batch [10]#011Speed: 6016.53 samples/sec#011loss=3.905954\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] processed a total of 1527 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565011.5636275, \"EndTime\": 1680565011.9941003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 429.9776554107666, \"count\": 1, \"min\": 429.9776554107666, \"max\": 429.9776554107666}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3550.189827859061 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:51 INFO 140244386916160] #quality_metric: host=algo-1, epoch=147, train loss <loss>=3.9520957271258035\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[148] Batch[0] avg_epoch_loss=3.510531\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=3.510530948638916\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[148] Batch[5] avg_epoch_loss=3.874153\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=3.874152580897013\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[148] Batch [5]#011Speed: 8217.00 samples/sec#011loss=3.874153\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[148] Batch[10] avg_epoch_loss=3.920732\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=3.9766283512115477\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[148] Batch [10]#011Speed: 5705.60 samples/sec#011loss=3.976628\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] processed a total of 1472 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565011.9941895, \"EndTime\": 1680565012.4338834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 439.05186653137207, \"count\": 1, \"min\": 439.05186653137207, \"max\": 439.05186653137207}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3351.567039858251 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #progress_metric: host=algo-1, completed 74.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=148, train loss <loss>=3.7970351775487265\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[149] Batch[0] avg_epoch_loss=3.688710\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=3.6887102127075195\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[149] Batch[5] avg_epoch_loss=4.072770\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=4.072770436604817\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[149] Batch [5]#011Speed: 9764.95 samples/sec#011loss=4.072770\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[149] Batch[10] avg_epoch_loss=4.043883\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=4.009218645095825\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] Epoch[149] Batch [10]#011Speed: 5679.12 samples/sec#011loss=4.009219\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] processed a total of 1520 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565012.433982, \"EndTime\": 1680565012.9130673, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 478.67751121520996, \"count\": 1, \"min\": 478.67751121520996, \"max\": 478.67751121520996}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3174.262953285323 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:52 INFO 140244386916160] #quality_metric: host=algo-1, epoch=149, train loss <loss>=4.169696946938832\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[150] Batch[0] avg_epoch_loss=3.734510\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=3.7345101833343506\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[150] Batch[5] avg_epoch_loss=3.669725\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=3.669724623362223\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[150] Batch [5]#011Speed: 9554.53 samples/sec#011loss=3.669725\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[150] Batch[10] avg_epoch_loss=3.612853\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=3.544607734680176\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[150] Batch [10]#011Speed: 6189.31 samples/sec#011loss=3.544608\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] processed a total of 1497 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565012.9131782, \"EndTime\": 1680565013.327188, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 413.5468006134033, \"count\": 1, \"min\": 413.5468006134033, \"max\": 413.5468006134033}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3618.757222531844 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #progress_metric: host=algo-1, completed 75.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=150, train loss <loss>=4.023512760798137\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[151] Batch[0] avg_epoch_loss=3.834578\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=3.834578037261963\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[151] Batch[5] avg_epoch_loss=3.570921\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=3.5709213415781655\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[151] Batch [5]#011Speed: 9581.68 samples/sec#011loss=3.570921\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[151] Batch[10] avg_epoch_loss=4.047231\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=4.6188023567199705\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[151] Batch [10]#011Speed: 5860.38 samples/sec#011loss=4.618802\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] processed a total of 1518 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565013.3272693, \"EndTime\": 1680565013.7538745, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 426.1491298675537, \"count\": 1, \"min\": 426.1491298675537, \"max\": 426.1491298675537}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3560.9717950180707 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=151, train loss <loss>=4.06653360525767\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] Epoch[152] Batch[0] avg_epoch_loss=3.682624\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:53 INFO 140244386916160] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=3.682623863220215\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[152] Batch[5] avg_epoch_loss=3.524138\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=3.5241376956303916\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[152] Batch [5]#011Speed: 9671.82 samples/sec#011loss=3.524138\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[152] Batch[10] avg_epoch_loss=3.768709\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=4.062194061279297\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[152] Batch [10]#011Speed: 5661.74 samples/sec#011loss=4.062194\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] processed a total of 1548 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565013.753963, \"EndTime\": 1680565014.1934185, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 438.9684200286865, \"count\": 1, \"min\": 438.9684200286865, \"max\": 438.9684200286865}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3525.122752000695 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #progress_metric: host=algo-1, completed 76.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=152, train loss <loss>=3.618808631713574\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[153] Batch[0] avg_epoch_loss=4.107704\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=4.107703685760498\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[153] Batch[5] avg_epoch_loss=3.995854\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=3.995854059855143\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[153] Batch [5]#011Speed: 9550.42 samples/sec#011loss=3.995854\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[153] Batch[10] avg_epoch_loss=3.990523\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=3.9841254711151124\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[153] Batch [10]#011Speed: 5425.42 samples/sec#011loss=3.984125\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] processed a total of 1589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565014.1935225, \"EndTime\": 1680565014.6317794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 437.746524810791, \"count\": 1, \"min\": 437.746524810791, \"max\": 437.746524810791}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3628.8181237568856 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #progress_metric: host=algo-1, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=153, train loss <loss>=4.495721835356492\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[154] Batch[0] avg_epoch_loss=4.812172\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=4.8121724128723145\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[154] Batch[5] avg_epoch_loss=3.897897\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=3.8978973627090454\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:54 INFO 140244386916160] Epoch[154] Batch [5]#011Speed: 9611.11 samples/sec#011loss=3.897897\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[154] Batch[10] avg_epoch_loss=4.089583\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=4.3196052551269535\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[154] Batch [10]#011Speed: 5354.07 samples/sec#011loss=4.319605\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] processed a total of 1543 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565014.6318688, \"EndTime\": 1680565015.0786211, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 446.2099075317383, \"count\": 1, \"min\": 446.2099075317383, \"max\": 446.2099075317383}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3457.0901647400697 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #progress_metric: host=algo-1, completed 77.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=154, train loss <loss>=3.869482923012513\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[155] Batch[0] avg_epoch_loss=4.332859\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=4.332858562469482\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[155] Batch[5] avg_epoch_loss=3.922362\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=3.9223617712656655\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[155] Batch [5]#011Speed: 9105.25 samples/sec#011loss=3.922362\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[155] Batch[10] avg_epoch_loss=3.919814\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=3.916757345199585\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[155] Batch [10]#011Speed: 4966.78 samples/sec#011loss=3.916757\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] processed a total of 1556 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565015.0786896, \"EndTime\": 1680565015.5324655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 453.2182216644287, \"count\": 1, \"min\": 453.2182216644287, \"max\": 453.2182216644287}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3431.1688314965663 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=155, train loss <loss>=3.7215291995268602\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[156] Batch[0] avg_epoch_loss=2.895496\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=2.895495891571045\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[156] Batch[5] avg_epoch_loss=3.477851\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=3.477851231892904\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[156] Batch [5]#011Speed: 9547.66 samples/sec#011loss=3.477851\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[156] Batch[10] avg_epoch_loss=3.661349\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=3.881546640396118\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] Epoch[156] Batch [10]#011Speed: 6049.97 samples/sec#011loss=3.881547\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] processed a total of 1511 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565015.532643, \"EndTime\": 1680565015.9576836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 424.00360107421875, \"count\": 1, \"min\": 424.00360107421875, \"max\": 424.00360107421875}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3562.4170363937346 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #progress_metric: host=algo-1, completed 78.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:55 INFO 140244386916160] #quality_metric: host=algo-1, epoch=156, train loss <loss>=3.788598279158274\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[157] Batch[0] avg_epoch_loss=3.547046\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=3.547046422958374\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[157] Batch[5] avg_epoch_loss=3.964219\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=3.9642193714777627\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[157] Batch [5]#011Speed: 9668.02 samples/sec#011loss=3.964219\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[157] Batch[10] avg_epoch_loss=3.762811\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=3.5211207389831545\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[157] Batch [10]#011Speed: 5061.86 samples/sec#011loss=3.521121\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] processed a total of 1519 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565015.957779, \"EndTime\": 1680565016.3975396, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 439.29314613342285, \"count\": 1, \"min\": 439.29314613342285, \"max\": 439.29314613342285}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3456.5745148239716 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #progress_metric: host=algo-1, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=157, train loss <loss>=3.9428824385007224\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[158] Batch[0] avg_epoch_loss=4.275524\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=4.275524139404297\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[158] Batch[5] avg_epoch_loss=3.945681\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=3.945680578549703\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[158] Batch [5]#011Speed: 7796.81 samples/sec#011loss=3.945681\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[158] Batch[10] avg_epoch_loss=3.911933\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=3.871435022354126\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] Epoch[158] Batch [10]#011Speed: 5845.97 samples/sec#011loss=3.871435\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] processed a total of 1517 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565016.397647, \"EndTime\": 1680565016.8441775, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 445.9717273712158, \"count\": 1, \"min\": 445.9717273712158, \"max\": 445.9717273712158}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3400.36659295275 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #progress_metric: host=algo-1, completed 79.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:56 INFO 140244386916160] #quality_metric: host=algo-1, epoch=158, train loss <loss>=3.9169187347094216\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[159] Batch[0] avg_epoch_loss=3.171581\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=3.171581268310547\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[159] Batch[5] avg_epoch_loss=3.787380\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=3.78738001982371\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[159] Batch [5]#011Speed: 9439.95 samples/sec#011loss=3.787380\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[159] Batch[10] avg_epoch_loss=3.945523\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=4.135293960571289\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[159] Batch [10]#011Speed: 5460.27 samples/sec#011loss=4.135294\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] processed a total of 1516 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565016.8442798, \"EndTime\": 1680565017.2803128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 435.5206489562988, \"count\": 1, \"min\": 435.5206489562988, \"max\": 435.5206489562988}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3479.02286551253 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=159, train loss <loss>=3.9694241881370544\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[160] Batch[0] avg_epoch_loss=4.215934\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=4.215933799743652\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[160] Batch[5] avg_epoch_loss=4.046659\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=4.0466586748758955\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[160] Batch [5]#011Speed: 9064.33 samples/sec#011loss=4.046659\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[160] Batch[10] avg_epoch_loss=4.012030\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=3.970475435256958\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[160] Batch [10]#011Speed: 6190.04 samples/sec#011loss=3.970475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] processed a total of 1487 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565017.28048, \"EndTime\": 1680565017.7062695, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 425.2734184265137, \"count\": 1, \"min\": 425.2734184265137, \"max\": 425.2734184265137}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3495.364987561248 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #progress_metric: host=algo-1, completed 80.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=160, train loss <loss>=3.9252896507581077\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] Epoch[161] Batch[0] avg_epoch_loss=3.484797\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:57 INFO 140244386916160] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=3.4847967624664307\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[161] Batch[5] avg_epoch_loss=3.695533\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=3.695532520612081\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[161] Batch [5]#011Speed: 7714.57 samples/sec#011loss=3.695533\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[161] Batch[10] avg_epoch_loss=3.855032\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=4.046430826187134\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[161] Batch [10]#011Speed: 5380.95 samples/sec#011loss=4.046431\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] processed a total of 1496 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565017.7063673, \"EndTime\": 1680565018.156768, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.8739242553711, \"count\": 1, \"min\": 449.8739242553711, \"max\": 449.8739242553711}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3324.44937624309 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #progress_metric: host=algo-1, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=161, train loss <loss>=4.126395622889201\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[162] Batch[0] avg_epoch_loss=2.720107\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=2.720106601715088\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[162] Batch[5] avg_epoch_loss=3.485733\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=3.485732833544413\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[162] Batch [5]#011Speed: 9646.48 samples/sec#011loss=3.485733\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[162] Batch[10] avg_epoch_loss=3.725325\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=4.012834787368774\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[162] Batch [10]#011Speed: 6229.47 samples/sec#011loss=4.012835\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] processed a total of 1512 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565018.1568441, \"EndTime\": 1680565018.5675004, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 410.11619567871094, \"count\": 1, \"min\": 410.11619567871094, \"max\": 410.11619567871094}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3685.4381081946503 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #progress_metric: host=algo-1, completed 81.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=162, train loss <loss>=3.795895973841349\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[163] Batch[0] avg_epoch_loss=4.699218\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=4.699217796325684\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[163] Batch[5] avg_epoch_loss=4.567629\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=4.567628582318624\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[163] Batch [5]#011Speed: 9546.37 samples/sec#011loss=4.567629\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[163] Batch[10] avg_epoch_loss=4.164966\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=3.681770896911621\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:58 INFO 140244386916160] Epoch[163] Batch [10]#011Speed: 4485.10 samples/sec#011loss=3.681771\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] processed a total of 1586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565018.5675955, \"EndTime\": 1680565019.0276568, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 459.5620632171631, \"count\": 1, \"min\": 459.5620632171631, \"max\": 459.5620632171631}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3450.009124766163 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=163, train loss <loss>=3.910286334844736\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[164] Batch[0] avg_epoch_loss=3.781051\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=3.7810511589050293\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[164] Batch[5] avg_epoch_loss=3.825909\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=3.8259094953536987\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[164] Batch [5]#011Speed: 9516.96 samples/sec#011loss=3.825909\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[164] Batch[10] avg_epoch_loss=3.867214\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=164, batch=10 train loss <loss>=3.9167787075042724\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[164] Batch [10]#011Speed: 5110.28 samples/sec#011loss=3.916779\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] processed a total of 1566 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565019.0277512, \"EndTime\": 1680565019.4739835, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 445.73283195495605, \"count\": 1, \"min\": 445.73283195495605, \"max\": 445.73283195495605}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3511.6699042725813 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #progress_metric: host=algo-1, completed 82.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=164, train loss <loss>=3.896542035616361\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[165] Batch[0] avg_epoch_loss=3.304771\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=3.3047709465026855\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[165] Batch[5] avg_epoch_loss=3.581592\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=3.581591804822286\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[165] Batch [5]#011Speed: 8343.42 samples/sec#011loss=3.581592\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[165] Batch[10] avg_epoch_loss=3.914995\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=4.315078401565552\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] Epoch[165] Batch [10]#011Speed: 5536.74 samples/sec#011loss=4.315078\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] processed a total of 1461 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565019.4741416, \"EndTime\": 1680565019.9263864, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 451.6909122467041, \"count\": 1, \"min\": 451.6909122467041, \"max\": 451.6909122467041}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3233.558623135179 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #progress_metric: host=algo-1, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:36:59 INFO 140244386916160] #quality_metric: host=algo-1, epoch=165, train loss <loss>=3.7479035556316376\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[166] Batch[0] avg_epoch_loss=5.261674\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=5.261673927307129\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[166] Batch[5] avg_epoch_loss=4.124956\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=4.124956210454305\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[166] Batch [5]#011Speed: 9261.25 samples/sec#011loss=4.124956\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[166] Batch[10] avg_epoch_loss=4.028805\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=3.913423204421997\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[166] Batch [10]#011Speed: 5861.07 samples/sec#011loss=3.913423\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] processed a total of 1508 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565019.926475, \"EndTime\": 1680565020.3787348, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 451.8313407897949, \"count\": 1, \"min\": 451.8313407897949, \"max\": 451.8313407897949}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3336.414134556701 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #progress_metric: host=algo-1, completed 83.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=166, train loss <loss>=4.05207775036494\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[167] Batch[0] avg_epoch_loss=4.669559\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=4.669558525085449\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[167] Batch[5] avg_epoch_loss=4.205370\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=4.205370306968689\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[167] Batch [5]#011Speed: 9360.29 samples/sec#011loss=4.205370\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[167] Batch[10] avg_epoch_loss=3.990656\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=3.7329991340637205\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] Epoch[167] Batch [10]#011Speed: 5288.39 samples/sec#011loss=3.732999\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] processed a total of 1522 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565020.378834, \"EndTime\": 1680565020.8142402, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 434.91458892822266, \"count\": 1, \"min\": 434.91458892822266, \"max\": 434.91458892822266}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3498.1512214180934 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:00 INFO 140244386916160] #quality_metric: host=algo-1, epoch=167, train loss <loss>=3.996422747770945\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[168] Batch[0] avg_epoch_loss=3.327693\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=3.327692985534668\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[168] Batch[5] avg_epoch_loss=3.631569\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=3.6315694650014243\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[168] Batch [5]#011Speed: 9558.61 samples/sec#011loss=3.631569\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[168] Batch[10] avg_epoch_loss=3.681227\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=3.7408161640167235\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[168] Batch [10]#011Speed: 5248.30 samples/sec#011loss=3.740816\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] processed a total of 1523 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565020.8143647, \"EndTime\": 1680565021.2495303, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 434.77439880371094, \"count\": 1, \"min\": 434.77439880371094, \"max\": 434.77439880371094}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3501.825483370519 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #progress_metric: host=algo-1, completed 84.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=168, train loss <loss>=3.8142127792040506\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[169] Batch[0] avg_epoch_loss=3.458139\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=3.4581387042999268\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[169] Batch[5] avg_epoch_loss=3.675085\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=3.675084869066874\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[169] Batch [5]#011Speed: 7764.60 samples/sec#011loss=3.675085\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[169] Batch[10] avg_epoch_loss=3.864418\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=4.091617679595947\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[169] Batch [10]#011Speed: 4758.88 samples/sec#011loss=4.091618\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] processed a total of 1531 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565021.249622, \"EndTime\": 1680565021.708512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 458.40954780578613, \"count\": 1, \"min\": 458.40954780578613, \"max\": 458.40954780578613}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3338.7715860086746 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=169, train loss <loss>=3.9546875754992166\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] Epoch[170] Batch[0] avg_epoch_loss=3.850225\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:01 INFO 140244386916160] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=3.8502254486083984\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[170] Batch[5] avg_epoch_loss=4.114321\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=4.114320874214172\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[170] Batch [5]#011Speed: 6774.07 samples/sec#011loss=4.114321\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[170] Batch[10] avg_epoch_loss=4.037339\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=3.944961357116699\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[170] Batch [10]#011Speed: 5675.86 samples/sec#011loss=3.944961\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] processed a total of 1477 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565021.7086072, \"EndTime\": 1680565022.2122111, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 503.1905174255371, \"count\": 1, \"min\": 503.1905174255371, \"max\": 503.1905174255371}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2934.099377846485 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #progress_metric: host=algo-1, completed 85.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=170, train loss <loss>=3.975094656149546\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[171] Batch[0] avg_epoch_loss=3.788390\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=3.7883899211883545\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[171] Batch[5] avg_epoch_loss=4.130137\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=4.130137125651042\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[171] Batch [5]#011Speed: 8140.62 samples/sec#011loss=4.130137\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[171] Batch[10] avg_epoch_loss=3.976807\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=3.7928105354309083\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] Epoch[171] Batch [10]#011Speed: 4364.59 samples/sec#011loss=3.792811\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] processed a total of 1512 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565022.212362, \"EndTime\": 1680565022.7403016, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.4176597595215, \"count\": 1, \"min\": 527.4176597595215, \"max\": 527.4176597595215}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2866.026036427136 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:02 INFO 140244386916160] #quality_metric: host=algo-1, epoch=171, train loss <loss>=4.023505131403605\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[172] Batch[0] avg_epoch_loss=3.912781\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=3.912781000137329\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[172] Batch[5] avg_epoch_loss=3.849160\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=3.849159518877665\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[172] Batch [5]#011Speed: 6758.91 samples/sec#011loss=3.849160\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[172] Batch[10] avg_epoch_loss=3.934078\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=4.035980749130249\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[172] Batch [10]#011Speed: 4107.97 samples/sec#011loss=4.035981\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] processed a total of 1528 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565022.740396, \"EndTime\": 1680565023.2786708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 537.8384590148926, \"count\": 1, \"min\": 537.8384590148926, \"max\": 537.8384590148926}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2840.1984811849875 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #progress_metric: host=algo-1, completed 86.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=172, train loss <loss>=3.996962308883667\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[173] Batch[0] avg_epoch_loss=3.815115\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=3.815114736557007\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[173] Batch[5] avg_epoch_loss=3.681581\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=3.681580901145935\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[173] Batch [5]#011Speed: 6766.08 samples/sec#011loss=3.681581\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[173] Batch[10] avg_epoch_loss=3.825490\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=173, batch=10 train loss <loss>=3.9981809139251707\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] Epoch[173] Batch [10]#011Speed: 4438.42 samples/sec#011loss=3.998181\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] processed a total of 1476 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565023.2787726, \"EndTime\": 1680565023.8541596, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.9318599700928, \"count\": 1, \"min\": 574.9318599700928, \"max\": 574.9318599700928}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2566.6189494212786 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #progress_metric: host=algo-1, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:03 INFO 140244386916160] #quality_metric: host=algo-1, epoch=173, train loss <loss>=4.446062803268433\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[174] Batch[0] avg_epoch_loss=2.959740\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=2.959740161895752\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[174] Batch[5] avg_epoch_loss=3.748722\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=3.748721798261007\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[174] Batch [5]#011Speed: 7794.72 samples/sec#011loss=3.748722\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[174] Batch[10] avg_epoch_loss=3.881539\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=174, batch=10 train loss <loss>=4.0409198760986325\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[174] Batch [10]#011Speed: 6060.89 samples/sec#011loss=4.040920\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] processed a total of 1486 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565023.854258, \"EndTime\": 1680565024.441132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 586.4546298980713, \"count\": 1, \"min\": 586.4546298980713, \"max\": 586.4546298980713}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2533.165943019436 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #progress_metric: host=algo-1, completed 87.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=174, train loss <loss>=3.768969178199768\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[175] Batch[0] avg_epoch_loss=3.798953\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=3.7989532947540283\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[175] Batch[5] avg_epoch_loss=3.861718\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=3.8617177406946817\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[175] Batch [5]#011Speed: 9510.76 samples/sec#011loss=3.861718\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[175] Batch[10] avg_epoch_loss=3.931174\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=4.014521360397339\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] Epoch[175] Batch [10]#011Speed: 6374.75 samples/sec#011loss=4.014521\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] processed a total of 1472 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565024.4412413, \"EndTime\": 1680565024.8537319, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 412.017822265625, \"count\": 1, \"min\": 412.017822265625, \"max\": 412.017822265625}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3571.6007476393306 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:04 INFO 140244386916160] #quality_metric: host=algo-1, epoch=175, train loss <loss>=4.100106318791707\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[176] Batch[0] avg_epoch_loss=4.097986\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=4.097985744476318\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[176] Batch[5] avg_epoch_loss=3.734012\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=3.7340116103490195\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[176] Batch [5]#011Speed: 9542.88 samples/sec#011loss=3.734012\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[176] Batch[10] avg_epoch_loss=3.851688\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=3.992899227142334\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[176] Batch [10]#011Speed: 4315.54 samples/sec#011loss=3.992899\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] processed a total of 1594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565024.8538067, \"EndTime\": 1680565025.318929, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.6317958831787, \"count\": 1, \"min\": 464.6317958831787, \"max\": 464.6317958831787}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3428.9544770087355 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #progress_metric: host=algo-1, completed 88.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=176, train loss <loss>=3.851986499933096\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[177] Batch[0] avg_epoch_loss=4.331365\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=4.33136510848999\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[177] Batch[5] avg_epoch_loss=3.483442\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=3.4834423065185547\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[177] Batch [5]#011Speed: 9469.19 samples/sec#011loss=3.483442\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[177] Batch[10] avg_epoch_loss=3.630895\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=3.807839107513428\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[177] Batch [10]#011Speed: 5418.00 samples/sec#011loss=3.807839\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] processed a total of 1521 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565025.3191109, \"EndTime\": 1680565025.762417, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 442.8730010986328, \"count\": 1, \"min\": 442.8730010986328, \"max\": 442.8730010986328}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3433.1733135866675 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #progress_metric: host=algo-1, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=177, train loss <loss>=3.7037269671758017\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] Epoch[178] Batch[0] avg_epoch_loss=4.065451\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:05 INFO 140244386916160] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=4.065451145172119\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[178] Batch[5] avg_epoch_loss=3.258051\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=3.2580509583155313\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[178] Batch [5]#011Speed: 7712.55 samples/sec#011loss=3.258051\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[178] Batch[10] avg_epoch_loss=3.574222\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=3.953626298904419\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[178] Batch [10]#011Speed: 5189.28 samples/sec#011loss=3.953626\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] processed a total of 1530 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565025.7625227, \"EndTime\": 1680565026.2163808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 453.4029960632324, \"count\": 1, \"min\": 453.4029960632324, \"max\": 453.4029960632324}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3373.321362262953 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #progress_metric: host=algo-1, completed 89.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=178, train loss <loss>=3.7346838315327964\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[179] Batch[0] avg_epoch_loss=4.101145\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=4.101145267486572\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[179] Batch[5] avg_epoch_loss=3.917706\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=3.9177058140436807\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[179] Batch [5]#011Speed: 9528.14 samples/sec#011loss=3.917706\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[179] Batch[10] avg_epoch_loss=4.003735\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=4.106969833374023\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[179] Batch [10]#011Speed: 6637.97 samples/sec#011loss=4.106970\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] processed a total of 1457 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565026.2164845, \"EndTime\": 1680565026.6332445, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 416.2611961364746, \"count\": 1, \"min\": 416.2611961364746, \"max\": 416.2611961364746}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3498.731253098768 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=179, train loss <loss>=3.9626904924710593\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[180] Batch[0] avg_epoch_loss=3.795585\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=3.7955853939056396\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[180] Batch[5] avg_epoch_loss=4.010818\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=4.010818123817444\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:06 INFO 140244386916160] Epoch[180] Batch [5]#011Speed: 8340.57 samples/sec#011loss=4.010818\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[180] Batch[10] avg_epoch_loss=4.261688\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=4.562732315063476\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[180] Batch [10]#011Speed: 4672.25 samples/sec#011loss=4.562732\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] processed a total of 1624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565026.6333597, \"EndTime\": 1680565027.1096427, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 475.8274555206299, \"count\": 1, \"min\": 475.8274555206299, \"max\": 475.8274555206299}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3411.6821756872023 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #progress_metric: host=algo-1, completed 90.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=180, train loss <loss>=4.130672234755296\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[181] Batch[0] avg_epoch_loss=3.765124\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=3.7651243209838867\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[181] Batch[5] avg_epoch_loss=3.842354\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=3.8423543771107993\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[181] Batch [5]#011Speed: 9212.97 samples/sec#011loss=3.842354\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[181] Batch[10] avg_epoch_loss=3.870224\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=181, batch=10 train loss <loss>=3.903667688369751\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[181] Batch [10]#011Speed: 5297.49 samples/sec#011loss=3.903668\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] processed a total of 1503 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565027.1097736, \"EndTime\": 1680565027.546585, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 436.30075454711914, \"count\": 1, \"min\": 436.30075454711914, \"max\": 436.30075454711914}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3443.6783501910836 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #progress_metric: host=algo-1, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=181, train loss <loss>=3.8950305581092834\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[182] Batch[0] avg_epoch_loss=4.466500\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=4.466500282287598\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[182] Batch[5] avg_epoch_loss=3.980029\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=3.9800287087758384\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[182] Batch [5]#011Speed: 9756.11 samples/sec#011loss=3.980029\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[182] Batch[10] avg_epoch_loss=4.154060\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=4.362898302078247\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] Epoch[182] Batch [10]#011Speed: 5102.04 samples/sec#011loss=4.362898\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] processed a total of 1587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565027.5466921, \"EndTime\": 1680565027.9974172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.23465156555176, \"count\": 1, \"min\": 450.23465156555176, \"max\": 450.23465156555176}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3523.074969791374 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #progress_metric: host=algo-1, completed 91.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:07 INFO 140244386916160] #quality_metric: host=algo-1, epoch=182, train loss <loss>=3.9229106719677267\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[183] Batch[0] avg_epoch_loss=3.404746\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=3.4047462940216064\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[183] Batch[5] avg_epoch_loss=3.754217\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=3.7542172273000083\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[183] Batch [5]#011Speed: 9643.05 samples/sec#011loss=3.754217\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[183] Batch[10] avg_epoch_loss=3.758490\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=3.763616752624512\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[183] Batch [10]#011Speed: 5219.46 samples/sec#011loss=3.763617\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] processed a total of 1501 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565027.9975178, \"EndTime\": 1680565028.4450362, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 447.10516929626465, \"count\": 1, \"min\": 447.10516929626465, \"max\": 447.10516929626465}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3356.1264993059212 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=183, train loss <loss>=3.8864485820134482\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[184] Batch[0] avg_epoch_loss=4.508208\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=4.50820779800415\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[184] Batch[5] avg_epoch_loss=3.598486\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=3.5984862645467124\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[184] Batch [5]#011Speed: 8869.62 samples/sec#011loss=3.598486\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[184] Batch[10] avg_epoch_loss=3.997994\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=184, batch=10 train loss <loss>=4.4774041175842285\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] Epoch[184] Batch [10]#011Speed: 5554.91 samples/sec#011loss=4.477404\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] processed a total of 1462 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565028.4451263, \"EndTime\": 1680565028.8830636, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 437.4241828918457, \"count\": 1, \"min\": 437.4241828918457, \"max\": 437.4241828918457}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3340.853132230734 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #progress_metric: host=algo-1, completed 92.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:08 INFO 140244386916160] #quality_metric: host=algo-1, epoch=184, train loss <loss>=3.930220286051432\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[185] Batch[0] avg_epoch_loss=5.326963\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=5.326963424682617\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[185] Batch[5] avg_epoch_loss=3.827145\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=3.827145497004191\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[185] Batch [5]#011Speed: 8703.12 samples/sec#011loss=3.827145\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[185] Batch[10] avg_epoch_loss=3.834277\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=3.842835521697998\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[185] Batch [10]#011Speed: 4885.95 samples/sec#011loss=3.842836\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] processed a total of 1502 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565028.883205, \"EndTime\": 1680565029.395911, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 512.1486186981201, \"count\": 1, \"min\": 512.1486186981201, \"max\": 512.1486186981201}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2931.933079536464 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #progress_metric: host=algo-1, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=185, train loss <loss>=3.8331167300542197\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[186] Batch[0] avg_epoch_loss=4.078375\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=4.078374862670898\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[186] Batch[5] avg_epoch_loss=3.519164\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=3.519164045651754\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[186] Batch [5]#011Speed: 8081.29 samples/sec#011loss=3.519164\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[186] Batch[10] avg_epoch_loss=3.830832\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=4.204834604263306\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] Epoch[186] Batch [10]#011Speed: 5712.36 samples/sec#011loss=4.204835\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] processed a total of 1554 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565029.3959992, \"EndTime\": 1680565029.85476, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 458.3461284637451, \"count\": 1, \"min\": 458.3461284637451, \"max\": 458.3461284637451}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3388.8255302555676 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #progress_metric: host=algo-1, completed 93.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:09 INFO 140244386916160] #quality_metric: host=algo-1, epoch=186, train loss <loss>=5.062431775606596\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[187] Batch[0] avg_epoch_loss=3.989369\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=3.9893693923950195\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[187] Batch[5] avg_epoch_loss=3.835930\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=3.8359301884969077\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[187] Batch [5]#011Speed: 7987.01 samples/sec#011loss=3.835930\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[187] Batch[10] avg_epoch_loss=4.009962\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=4.218799304962158\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[187] Batch [10]#011Speed: 5691.76 samples/sec#011loss=4.218799\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] processed a total of 1484 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565029.8549278, \"EndTime\": 1680565030.2995038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 443.9377784729004, \"count\": 1, \"min\": 443.9377784729004, \"max\": 443.9377784729004}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3341.733984033158 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=187, train loss <loss>=3.9830297430356345\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[188] Batch[0] avg_epoch_loss=3.120957\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=3.120957374572754\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[188] Batch[5] avg_epoch_loss=3.845484\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=3.8454840183258057\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[188] Batch [5]#011Speed: 9582.77 samples/sec#011loss=3.845484\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[188] Batch[10] avg_epoch_loss=4.030056\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=4.251542949676514\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[188] Batch [10]#011Speed: 5796.62 samples/sec#011loss=4.251543\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] processed a total of 1542 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565030.2995982, \"EndTime\": 1680565030.7448974, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 444.83041763305664, \"count\": 1, \"min\": 444.83041763305664, \"max\": 444.83041763305664}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3465.2804552066673 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #progress_metric: host=algo-1, completed 94.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=188, train loss <loss>=3.7829381112868967\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] Epoch[189] Batch[0] avg_epoch_loss=3.803449\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:10 INFO 140244386916160] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=3.8034486770629883\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[189] Batch[5] avg_epoch_loss=4.033179\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=4.033179004987081\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[189] Batch [5]#011Speed: 9468.12 samples/sec#011loss=4.033179\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[189] Batch[10] avg_epoch_loss=4.037068\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=4.041734838485718\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[189] Batch [10]#011Speed: 5602.31 samples/sec#011loss=4.041735\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] processed a total of 1463 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565030.7450035, \"EndTime\": 1680565031.178213, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 432.6667785644531, \"count\": 1, \"min\": 432.6667785644531, \"max\": 432.6667785644531}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3380.0642999533443 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=189, train loss <loss>=4.535911401112874\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[190] Batch[0] avg_epoch_loss=3.714103\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=3.7141027450561523\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[190] Batch[5] avg_epoch_loss=3.901697\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=3.901696960131327\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[190] Batch [5]#011Speed: 9512.88 samples/sec#011loss=3.901697\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[190] Batch[10] avg_epoch_loss=3.940845\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=3.9878217697143556\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[190] Batch [10]#011Speed: 5031.91 samples/sec#011loss=3.987822\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] processed a total of 1595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565031.1783164, \"EndTime\": 1680565031.6310484, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.21662521362305, \"count\": 1, \"min\": 452.21662521362305, \"max\": 452.21662521362305}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3525.9943298759877 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #progress_metric: host=algo-1, completed 95.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=190, train loss <loss>=3.8192147658421445\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[191] Batch[0] avg_epoch_loss=4.742845\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=4.742845058441162\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[191] Batch[5] avg_epoch_loss=3.437300\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=3.4373004039128623\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:11 INFO 140244386916160] Epoch[191] Batch [5]#011Speed: 9533.08 samples/sec#011loss=3.437300\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[191] Batch[10] avg_epoch_loss=3.555967\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=3.6983673572540283\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[191] Batch [10]#011Speed: 5136.68 samples/sec#011loss=3.698367\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] processed a total of 1558 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565031.6311374, \"EndTime\": 1680565032.099783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 468.151330947876, \"count\": 1, \"min\": 468.151330947876, \"max\": 468.151330947876}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3327.0941370423207 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=191, train loss <loss>=4.705687302809495\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[192] Batch[0] avg_epoch_loss=3.495188\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=3.4951884746551514\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[192] Batch[5] avg_epoch_loss=3.634217\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=3.634217460950216\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[192] Batch [5]#011Speed: 9417.33 samples/sec#011loss=3.634217\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[192] Batch[10] avg_epoch_loss=3.784128\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=3.9640204906463623\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[192] Batch [10]#011Speed: 4819.43 samples/sec#011loss=3.964020\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] processed a total of 1581 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565032.099859, \"EndTime\": 1680565032.5577724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 457.4460983276367, \"count\": 1, \"min\": 457.4460983276367, \"max\": 457.4460983276367}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3454.636049066791 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #progress_metric: host=algo-1, completed 96.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=192, train loss <loss>=3.750502219566932\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[193] Batch[0] avg_epoch_loss=4.156640\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=4.156639575958252\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[193] Batch[5] avg_epoch_loss=4.165759\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=4.1657586097717285\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:12 INFO 140244386916160] Epoch[193] Batch [5]#011Speed: 7681.59 samples/sec#011loss=4.165759\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[193] Batch[10] avg_epoch_loss=4.095957\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=193, batch=10 train loss <loss>=4.0121955394744875\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[193] Batch [10]#011Speed: 5286.11 samples/sec#011loss=4.012196\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] processed a total of 1479 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565032.557921, \"EndTime\": 1680565033.0194683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 460.9858989715576, \"count\": 1, \"min\": 460.9858989715576, \"max\": 460.9858989715576}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3207.4220805979517 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #progress_metric: host=algo-1, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=193, train loss <loss>=4.454394320646922\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[194] Batch[0] avg_epoch_loss=4.091589\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=4.091588973999023\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[194] Batch[5] avg_epoch_loss=4.052707\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=4.052707235018413\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[194] Batch [5]#011Speed: 7710.94 samples/sec#011loss=4.052707\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[194] Batch[10] avg_epoch_loss=3.920533\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=3.7619229793548583\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[194] Batch [10]#011Speed: 5054.22 samples/sec#011loss=3.761923\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] processed a total of 1530 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565033.0195515, \"EndTime\": 1680565033.4706836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.6981372833252, \"count\": 1, \"min\": 450.6981372833252, \"max\": 450.6981372833252}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3393.1753094409837 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #progress_metric: host=algo-1, completed 97.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=194, train loss <loss>=3.9671870470046997\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[195] Batch[0] avg_epoch_loss=3.893067\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=3.893066883087158\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[195] Batch[5] avg_epoch_loss=3.669292\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=3.669291615486145\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[195] Batch [5]#011Speed: 8720.62 samples/sec#011loss=3.669292\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[195] Batch[10] avg_epoch_loss=3.750322\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=3.8475592613220213\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] Epoch[195] Batch [10]#011Speed: 5503.78 samples/sec#011loss=3.847559\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] processed a total of 1525 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565033.4708333, \"EndTime\": 1680565033.9138951, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 442.4772262573242, \"count\": 1, \"min\": 442.4772262573242, \"max\": 442.4772262573242}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3445.4746964484157 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:13 INFO 140244386916160] #quality_metric: host=algo-1, epoch=195, train loss <loss>=4.031327744325002\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[196] Batch[0] avg_epoch_loss=3.418599\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=3.4185991287231445\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[196] Batch[5] avg_epoch_loss=3.637374\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=3.6373735268910727\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[196] Batch [5]#011Speed: 9455.85 samples/sec#011loss=3.637374\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[196] Batch[10] avg_epoch_loss=3.686045\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=3.7444510459899902\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[196] Batch [10]#011Speed: 5969.29 samples/sec#011loss=3.744451\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] processed a total of 1504 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565033.9139807, \"EndTime\": 1680565034.3478923, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 433.4855079650879, \"count\": 1, \"min\": 433.4855079650879, \"max\": 433.4855079650879}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3468.5874144638265 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #progress_metric: host=algo-1, completed 98.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=196, train loss <loss>=3.716544508934021\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[197] Batch[0] avg_epoch_loss=2.896722\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=2.896721601486206\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[197] Batch[5] avg_epoch_loss=4.035387\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=4.035386721293132\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[197] Batch [5]#011Speed: 8672.66 samples/sec#011loss=4.035387\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[197] Batch[10] avg_epoch_loss=4.060485\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=197, batch=10 train loss <loss>=4.090603590011597\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] Epoch[197] Batch [10]#011Speed: 5474.94 samples/sec#011loss=4.090604\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] processed a total of 1529 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565034.347965, \"EndTime\": 1680565034.7816823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 433.21752548217773, \"count\": 1, \"min\": 433.21752548217773, \"max\": 433.21752548217773}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3527.8496023042644 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #progress_metric: host=algo-1, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:14 INFO 140244386916160] #quality_metric: host=algo-1, epoch=197, train loss <loss>=4.07221132516861\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[198] Batch[0] avg_epoch_loss=3.563989\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=3.5639894008636475\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[198] Batch[5] avg_epoch_loss=3.458669\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=3.458669344584147\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[198] Batch [5]#011Speed: 8313.63 samples/sec#011loss=3.458669\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[198] Batch[10] avg_epoch_loss=3.823394\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=198, batch=10 train loss <loss>=4.2610640048980715\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[198] Batch [10]#011Speed: 5912.43 samples/sec#011loss=4.261064\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] processed a total of 1455 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565034.7818172, \"EndTime\": 1680565035.269961, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 487.67733573913574, \"count\": 1, \"min\": 487.67733573913574, \"max\": 487.67733573913574}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=2982.764507388103 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #progress_metric: host=algo-1, completed 99.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=198, train loss <loss>=3.6872217456499734\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[199] Batch[0] avg_epoch_loss=3.255663\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=3.2556633949279785\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[199] Batch[5] avg_epoch_loss=3.560457\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=3.5604567925135293\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[199] Batch [5]#011Speed: 9671.57 samples/sec#011loss=3.560457\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[199] Batch[10] avg_epoch_loss=3.807586\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=199, batch=10 train loss <loss>=4.104140329360962\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Epoch[199] Batch [10]#011Speed: 4879.54 samples/sec#011loss=4.104140\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] processed a total of 1547 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565035.270042, \"EndTime\": 1680565035.7267742, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.2509059906006, \"count\": 1, \"min\": 456.2509059906006, \"max\": 456.2509059906006}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #throughput_metric: host=algo-1, train throughput=3389.542833501193 records/second\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, epoch=199, train loss <loss>=3.5943173949535074\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Final loss: 3.5943173949535074 (occurred at epoch 199)\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #quality_metric: host=algo-1, train final_loss <loss>=3.5943173949535074\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 WARNING 140244386916160] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565035.7268727, \"EndTime\": 1680565035.7510104, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 22.983789443969727, \"count\": 1, \"min\": 22.983789443969727, \"max\": 22.983789443969727}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565035.7510667, \"EndTime\": 1680565035.7666404, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 38.664817810058594, \"count\": 1, \"min\": 38.664817810058594, \"max\": 38.664817810058594}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565035.7667184, \"EndTime\": 1680565035.7686281, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 1.86920166015625, \"count\": 1, \"min\": 1.86920166015625, \"max\": 1.86920166015625}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] #memory_usage::<batchbuffer> = 0.2294921875 mb\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:15 INFO 140244386916160] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565035.7686808, \"EndTime\": 1680565035.7702587, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.04458427429199219, \"count\": 1, \"min\": 0.04458427429199219, \"max\": 0.04458427429199219}}}\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/numpy/ma/core.py:2794: UserWarning: Warning: converting a masked element to nan.\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565035.7703357, \"EndTime\": 1680565036.061887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 291.6731834411621, \"count\": 1, \"min\": 291.6731834411621, \"max\": 291.6731834411621}}}\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, RMSE): 295178.3588815413\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, mean_absolute_QuantileLoss): 2812703.4296006947\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, mean_wQuantileLoss): 20.97179526605801\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.1]): 9.63331921528752\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.2]): 22.738903373267345\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.3]): 28.39992275947428\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.4]): 29.137628473537312\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.5]): 27.560059401808825\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.6]): 24.599783889630164\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.7]): 20.88216122485999\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.8]): 15.968430503972876\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #test_score (algo-1, wQuantileLoss[0.9]): 9.825948552683803\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #quality_metric: host=algo-1, test RMSE <loss>=295178.3588815413\u001b[0m\n",
      "\u001b[34m[04/03/2023 23:37:16 INFO 140244386916160] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=20.97179526605801\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680565036.0619633, \"EndTime\": 1680565036.0682185, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 6.162881851196289, \"count\": 1, \"min\": 6.162881851196289, \"max\": 6.162881851196289}, \"totaltime\": {\"sum\": 92508.63766670227, \"count\": 1, \"min\": 92508.63766670227, \"max\": 92508.63766670227}}}\u001b[0m\n",
      "\n",
      "2023-04-03 23:37:36 Uploading - Uploading generated training model\n",
      "2023-04-03 23:37:36 Completed - Training job completed\n",
      "Training seconds: 197\n",
      "Billable seconds: 197\n",
      "CPU times: user 1.28 s, sys: 147 ms, total: 1.42 s\n",
      "Wall time: 4min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train\".format(s3_data_path), \"test\": \"{}/test\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3c712-3c85-4761-9007-0f39aa4b5b3f",
   "metadata": {},
   "source": [
    "## Creating Endpoint to make predictions on newly fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe432457-3202-4992-ab37-0fdbe9179ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: deepar-hotel-2023-04-03-23-32-55-031\n",
      "INFO:sagemaker:Creating endpoint-config with name deepar-hotel-2023-04-03-23-32-55-031\n",
      "INFO:sagemaker:Creating endpoint with name deepar-hotel-2023-04-03-23-32-55-031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57a19320-4966-4333-988a-e8aff018fd17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "\n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "\n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        encoding=\"utf-8\",\n",
    "        num_samples=100,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "        content_type=\"application/json\",\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req, initial_args={\"ContentType\": content_type})\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "\n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "\n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.date_range(\n",
    "                start=prediction_times[k], freq=self.freq, periods=self.prediction_length\n",
    "            )\n",
    "            list_of_df.append(\n",
    "                pd.DataFrame(\n",
    "                    data=response_data[\"predictions\"][k][\"quantiles\"], index=prediction_index\n",
    "                )\n",
    "            )\n",
    "        return list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e681ed03-4fb4-4bee-8cc3-79c2cba02435",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "predictor.set_prediction_parameters('M', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dded25b9-86f6-4e8e-be30-52ae13e9437a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-d7257e5de371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_of_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mactual_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-769647827d0d>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, encoding, num_samples, quantiles, content_type)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mprediction_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ContentType\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-769647827d0d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mprediction_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ContentType\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "list_of_df = predictor.predict(train_json[:5], content_type=\"application/json\")\n",
    "actual_data = train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173bc1b-2611-4795-ab74-5adeb544e907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(len(list_of_df)):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    actual_data[k][-prediction_length - context_length :].plot(label=\"target\")\n",
    "    p10 = list_of_df[k][\"0.1\"]\n",
    "    p90 = list_of_df[k][\"0.9\"]\n",
    "    plt.fill_between(p10.index, p10, p90, color=\"y\", alpha=0.5, label=\"80% confidence interval\")\n",
    "    list_of_df[k][\"0.5\"].plot(label=\"prediction median\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf6c11-f82d-474e-a0e8-549b60a9b15e",
   "metadata": {},
   "source": [
    "## Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ad46828-1c6c-499d-9123-de52a11418f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: deepar-hotel-2023-04-03-23-32-55-031\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf80db3-b6f4-44c1-8d07-1bedc5148b2d",
   "metadata": {},
   "source": [
    "## Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b40fd88f-a041-42c1-a144-6643d9cfc0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
       "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
       "        \n",
       "<script>\n",
       "try {\n",
       "    els = document.getElementsByClassName(\"sm-command-button\");\n",
       "    els[0].click();\n",
       "}\n",
       "catch(err) {\n",
       "    // NoOp\n",
       "}    \n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74ab3b-3aeb-4749-b0f2-a96c8b9adc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
